{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Springboard Capstone 1 helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import glob\n",
    "import os.path\n",
    "from os import path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing function\n",
    "\n",
    "This function prepares text data for training. For most models, the text will be processed further at training time, but pre-processing can save time when training is iterated multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "# function to prepare text for NLP analysis\n",
    "def process_text(text, \n",
    "                 stemmer=None, \n",
    "                 regexstr=None, \n",
    "                 lowercase=True,\n",
    "                 removestop=False,\n",
    "                 verbose=True):\n",
    "    \"\"\"Helper function to pre-process text.\n",
    "        Combines several preprocessing steps: lowercase, \n",
    "            remove stop, regex text cleaning, stemming.\n",
    "        If savedpath is passed, then try to load saved processed text data\n",
    "            and return that instead of processing.\"\"\"\n",
    "        \n",
    "    if type(stemmer) == str:\n",
    "        if stemmer.lower() == 'porter':\n",
    "            stemmer = PorterStemmer()\n",
    "        elif stemmer.lower() == 'snowball':\n",
    "            stemmer = SnowballStemmer(language='english')\n",
    "        else:\n",
    "            stemmer = None\n",
    "            \n",
    "    # convert text list to pandas Series\n",
    "    if type(text) == list or type(text) == np.array:\n",
    "        processed = pd.Series(text)\n",
    "    else:\n",
    "        processed = text\n",
    "    \n",
    "    # make text lowercase\n",
    "    if lowercase == True:\n",
    "        if verbose: print('make text lowercase')\n",
    "        processed = processed.str.lower()\n",
    "        \n",
    "    # remove stop words\n",
    "    # NOTE: stop words w/ capitals not removed!\n",
    "    if removestop == True:\n",
    "        if verbose: print('remove stop words')\n",
    "        stopwords = sw.words(\"english\")\n",
    "        processed = processed.map(lambda text: ' '.join([word for word in text.split() if word not in stopwords]))\n",
    "        \n",
    "    # apply regex expression\n",
    "    if regexstr is not None:\n",
    "        if verbose: print('apply regex expression')\n",
    "        regex = re.compile(regexstr) \n",
    "        processed = processed.str.replace(regex,' ')\n",
    "        \n",
    "    # stemming\n",
    "    # NOTE: stemming makes all lowercase\n",
    "    if stemmer is not None:\n",
    "        if verbose: print('stemming')\n",
    "        processed = processed.map(lambda x: ' '.join([stemmer.stem(y) for y in x.split(' ')]))\n",
    "        \n",
    "    if verbose: print('done')\n",
    "         \n",
    "    return processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-processed text data\n",
    "\n",
    "This function will do one of two things: \n",
    "- If a specified csv file containing pre-processed text exists, then it will load it and return the contents as an array of text.\n",
    "- If the specified file does not exist, it will pre-process the given text, save that to the file, and return the pre-processed text.\n",
    "\n",
    "The purpose of this function is to cut down on the time spent processing text data by storing pre-processed text (given specified processing parameters) to a file for retrieval the next time it is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_text(text, pathstart, processkwargs):\n",
    "    \"\"\"If possible, load a csv file containing pre-processed text data,\n",
    "        otherwise process the text and save it to file for later loading.\"\"\"\n",
    "    \n",
    "    # create name for prepared text file\n",
    "    preppath = pathstart+' stem=%s, regex=%s, lower=%d, stop=%d.csv'%(\n",
    "        processkwargs['stemmer'], \n",
    "        'None' if processkwargs['regexstr'] is None else processkwargs['regexstr'].replace('\\\\','(sl)'), \n",
    "        processkwargs['lowercase'], \n",
    "        processkwargs['removestop'])\n",
    "    \n",
    "    # if prepared text file exists, just load it\n",
    "    if path.exists(preppath):\n",
    "#         print('reading prepped text file: ', preppath)\n",
    "        df = pd.read_csv(preppath, skip_blank_lines=False, na_filter=False)\n",
    "        \n",
    "    # file doesn't exist, so do the text prep and save the result for later loading\n",
    "    else:\n",
    "#         print('no prepped file, so reading original and prepping text...')\n",
    "        text = process_text(text, **processkwargs)\n",
    "        # create df\n",
    "        df = pd.DataFrame({'text':text})\n",
    "        # save prepared text file\n",
    "        df.to_csv(preppath, index=False, na_rep='NaN')\n",
    "        \n",
    "    # return the processed text data\n",
    "    return df['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the feature data.\n",
    "\n",
    "The comment data used in this analysis was prepared in three stages:\n",
    "\n",
    "- [acquired using Reddit Python API PRAW](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_collect_comments_v1.ipynb) from 12 subs. 8 of the subs are non-political, and 4 are political in nature. Models are trained on data for only one subreddit at a time, so that they are specialized to that subreddit.\n",
    "\n",
    "\n",
    "- The raw comment metadata was [processed using PCA to produce a single toxicity score](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_generate_PCA_score_v2.ipynb) based on the votes and number of replies. Toxicity score was calculated and normalized within each subreddit and then ranged between -5 and +5 to create a toxicity score comparable between subs. The toxicity score was then thresholded to generate binary \"toxic\" vs. \"not toxic\" labels for supervised model training. The threshold applied was: score <= -1 = \"toxic\", otherwise \"not toxic\". \n",
    "\n",
    "\n",
    "- [Features for training the models were generated](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_comment_create_model_features_v1.ipynb) and saved to two sample aligned feature files for each subreddit. These files are used by the models for input.\n",
    "\n",
    "**Note** This code is a little complicated because 1) for efficiency I'm using cached pre-processed text, and 2) sometimes after loading and processing there are samples with NaN values. To keep data aligned, I need to combine the two data dfs, remove the NaN samples, then split the df back into base and numeric and return those.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_data(subnames, srcdir, toxic_thresh=-1, text_prep_args=None):\n",
    "    \"\"\"Load and prep the feature data from two matched data files\"\"\"\n",
    "    \n",
    "    # load all data csvs for listed subs into dataframes \n",
    "    base_dfs = []\n",
    "    numeric_dfs = []\n",
    "    for sub in subnames:\n",
    "        base_dfs.append(pd.read_csv(srcdir+'features_text_'+sub+'.csv'))\n",
    "        numeric_dfs.append(pd.read_csv(srcdir+'features_doc2vec_'+sub+'.csv'))\n",
    "        \n",
    "    # concat all sub dfs into one for each data type\n",
    "    base_df = pd.concat(base_dfs, ignore_index=True)\n",
    "    numeric_df = pd.concat(numeric_dfs, ignore_index=True)\n",
    "    \n",
    "    # combine both dfs\n",
    "    df = pd.concat([base_df, numeric_df], axis=1, ignore_index=True)\n",
    "    df.columns = list(base_df.columns.values) + list(numeric_df.columns.values)\n",
    "    \n",
    "    # remove any columns with all nans\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    \n",
    "    # remove any samples with nans\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # pre-process text \n",
    "    if text_prep_args is not None:\n",
    "        pathstart = srcdir + 'processed_text_' + '_'.join(subnames)\n",
    "        df['text'] = load_processed_text(df['text'], pathstart, text_prep_args)  \n",
    "        \n",
    "    # remove any samples with nans\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # split dfs back into text and numeric features\n",
    "    base_df = df[base_df.columns]\n",
    "    numeric_df = df[numeric_df.columns]\n",
    "    \n",
    "    # add numeric metadata features from base df to numeric df\n",
    "    numeric_df['u_comment_karma'] = base_df['u_comment_karma']\n",
    "\n",
    "    # return base df (text and all comment metadata), numeric features, training label\n",
    "    return base_df['text'], numeric_df, np.where(base_df['pca_score']>toxic_thresh,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance sample frequencies in training samples\n",
    "\n",
    "The classifier may require balancing of sample frequencies between classes for best results. This function will up-sample to the specified number of samples per class.\n",
    "\n",
    "The balance_classes_sparse function does sample balancing with sparse matrices, such as vectorized BOW data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************\n",
    "# pandas dataframe version of balance classes\n",
    "#******************************************************\n",
    "def balance_classes_df(df, ycol, samples_per_class=None, verbose=False):\n",
    "    \"\"\"Equalize number of samples so that all classes have equal numbers of samples.\n",
    "    If samples_per_class==None, then upsample (randomly repeat) all classes to the largest class,\n",
    "      Otherwise, set samples for all classes to samples_per_class.\"\"\"\n",
    "    \n",
    "    if verbose: print('Balancing class sample frequencies:')\n",
    "    \n",
    "    # all class IDs\n",
    "    classes =  df[ycol].unique()\n",
    "    classes = classes[~np.isnan(classes)]\n",
    "    \n",
    "    # get class with max samples\n",
    "    if verbose: print('\\tOriginal sample frequencies:')\n",
    "    if samples_per_class is None:\n",
    "        samples_per_class = 0\n",
    "        for c in classes:\n",
    "            if verbose: print('\\t\\tclass:',c,'#samples:',(df[ycol]==c).sum())\n",
    "            samples_per_class = np.max([samples_per_class, (df[ycol]==c).sum()])\n",
    "    if verbose: print('\\tNew samples_per_class:',samples_per_class)\n",
    "            \n",
    "    # create a list of samples for each class with equal sample numbers \n",
    "    newdata = []\n",
    "    for c in classes:\n",
    "        newdata.append(df[df[ycol]==c].sample(samples_per_class, replace=True)) \n",
    "\n",
    "    return pd.concat(newdata)\n",
    "\n",
    "#******************************************************\n",
    "# numpy array version of balance_classes\n",
    "#******************************************************\n",
    "def balance_classes_np(X, y, samples_per_class=None, verbose=False):\n",
    "    \"\"\"Equalize number of samples so that all classes have equal numbers of samples.\n",
    "    If samples_per_class==None, then upsample (randomly repeat) all classes to the largest class,\n",
    "      Otherwise, set samples for all classes to samples_per_class.\"\"\"\n",
    "   \n",
    "    # cheaty code here, TODO: reimplement this with pure np \n",
    "    Xy_df = pd.concat([pd.DataFrame(X),pd.DataFrame(y)], axis=1,ignore_index=True)\n",
    "    Xy_df.columns = [str(n) for n in Xy_df.columns]\n",
    "    Xy_df.columns.values[-1] = 'y'\n",
    "    Xy_df_bal = balance_classes_df(Xy_df, 'y', \n",
    "                                   samples_per_class=samples_per_class, \n",
    "                                   verbose=verbose)\n",
    "    \n",
    "    # more cheaty: assumes only last column is y.\n",
    "    # This will break if y is not a single column \n",
    "    #   (for example a 1-hot encoding)\n",
    "    return Xy_df_bal.values[:,:-1], Xy_df_bal.values[:,-1]\n",
    "\n",
    "#******************************************************\n",
    "# numpy sparse array version of balance_classes\n",
    "#******************************************************\n",
    "from scipy.sparse import vstack, hstack\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "def balance_classes_sparse(X, y, samples_per_class=None, verbose=False):\n",
    "    \"\"\"Equalize number of samples so that all classes have equal numbers of samples.\n",
    "    If samples_per_class==None, then upsample (randomly repeat) all classes to the largest class,\n",
    "      Otherwise, set samples for all classes to samples_per_class.\"\"\"\n",
    "    \n",
    "    def get_samples(arr, numsamples):\n",
    "        if arr.shape[0] >= numsamples:\n",
    "            index = np.arange(arr.shape[0])\n",
    "            np.random.shuffle(index)\n",
    "            return arr[index[:numsamples],:]\n",
    "        else:\n",
    "            samples = arr.copy()\n",
    "            numrepeats = int(numsamples / arr.shape[0])\n",
    "            lastsize = numsamples % arr.shape[0]\n",
    "            for i in range(numrepeats-1):\n",
    "                samples = vstack([samples,arr])\n",
    "            if lastsize > 0:\n",
    "                index = np.arange(arr.shape[0])\n",
    "                np.random.shuffle(index)\n",
    "                samples = vstack([samples, arr[index[:lastsize],:]])\n",
    "            return samples   \n",
    "    \n",
    "    if verbose: \n",
    "        print('Balancing class sample frequencies:')\n",
    "        \n",
    "    # all class IDs\n",
    "    classes =  np.unique(y)\n",
    "    classes = classes[~np.isnan(classes)]\n",
    "    \n",
    "    # get class with max samples\n",
    "    if verbose: \n",
    "        print('\\tOriginal sample frequencies:')\n",
    "    if samples_per_class is None:\n",
    "        samples_per_class = 0\n",
    "        for c in classes:\n",
    "            if verbose: \n",
    "                print('\\t\\tclass:',c,'#samples:',(np.sum(y==c)))\n",
    "            samples_per_class = np.max([samples_per_class, np.sum(y==c)])\n",
    "    if verbose: \n",
    "        print('\\tNew samples_per_class:',samples_per_class)\n",
    "                              \n",
    "    # combine X and y\n",
    "    Xy = csr_matrix(hstack([X, csr_matrix(np.reshape(y, (-1, 1)))]))\n",
    "       \n",
    "    # create a list of samples for each class with equal sample numbers \n",
    "    newdata = None\n",
    "    for c in classes:\n",
    "        if newdata is None:\n",
    "            newdata = get_samples(Xy[y==c,:], samples_per_class)\n",
    "        else:\n",
    "            newdata = vstack([newdata, get_samples(Xy[y==c,:], samples_per_class)])\n",
    "            \n",
    "    if verbose:\n",
    "        print('\\ttotal balanced samples:',newdata.shape[0])\n",
    "            \n",
    "    return newdata[:,:-1], newdata[:,-1].toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log results of each model test\n",
    "\n",
    "This function logs the results of a model test to a CSV logfile. Every model notebook logs to the same file so that results can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path\n",
    "from os import path\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from time import time\n",
    "\n",
    "def log_model_results(logpath, modelname, subname, \n",
    "                      y_test, y_pred, time=0):\n",
    "    \"\"\"Write to CSV log file containing results of model train/test runs\"\"\"\n",
    "    \n",
    "    # write the header labels\n",
    "    if not os.path.exists(logpath):\n",
    "        labels = (['date','model','sub','num_nontoxic','num_toxic',\n",
    "                   'acc_nontoxic','acc_toxic','accuracy','precision',\n",
    "                   'recall','balanced_acc','F1','roc_auc','time_sec'])\n",
    "        with open(logpath, 'a', newline='') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "            csvwriter.writerow(labels)\n",
    "            \n",
    "    # create data row\n",
    "    row = [datetime.datetime.now().strftime('%y%m%d_%H%M%S'),\n",
    "          modelname, subname, \n",
    "          (y_test==0).sum(), (y_test==1).sum(),\n",
    "           '%1.3f'%(((y_test==0) & (y_test==y_pred)).sum()/(y_test==0).sum()),\n",
    "           '%1.3f'%(((y_test==1) & (y_test==y_pred)).sum()/(y_test==1).sum()),\n",
    "           '%1.3f'%(np.sum((y_pred==y_test))/y_test.shape[0]),\n",
    "           '%1.3f'%(precision_score(y_test, y_pred)),\n",
    "           '%1.3f'%(recall_score(y_test, y_pred)),\n",
    "           '%1.3f'%(balanced_accuracy_score(y_test, y_pred)),\n",
    "           '%1.3f'%(f1_score(y_test, y_pred)),\n",
    "           '%1.3f'%(roc_auc_score(y_test, y_pred)),\n",
    "           '%1.2f'%(time)\n",
    "          ]\n",
    "    # write the data row\n",
    "    with open(logpath, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "        csvwriter.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
