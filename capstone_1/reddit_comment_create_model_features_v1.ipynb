{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating features for Reddit comment toxicity classifier models\n",
    "### John Burt\n",
    "\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "This notebook reads the comment data and creates feature data files for training and testing the classifier models I'll be evaluating as toxic comment detectors. The feature data is split because not all classifier models will use all feature data.\n",
    "\n",
    "Input data:\n",
    "- comments collected for each sub, with PCA-based toxicity score added.\n",
    "- input dir: ./data_scored\n",
    "\n",
    "Output to:\n",
    "- two feature files, one containing the prepared comment text plus toxicity score, and the other containing Doc2Vec vectors for each comment sample. I have to split these up for space saving reasons.\n",
    "- output dir: ./data_for_models\n",
    "\n",
    "There will be three feature sets:\n",
    "\n",
    "- Comment metadata features: I will only include one feature from the metadata downloaded with each comment, the comment author's \"comment karma\", which is the mean comment score for all comments the user has posted. Other comment features were excluded because either they did not correlate with toxicity score, or they were correlative but only if the comment had been posted for a while. Time dependent associations are not helpful as features, since I want this classifier to be able to detect toxic comments shortly after they are posted. Comment karma, however, is a measure that exists at the time of the comment and can therefore be used as a feature for the models.\n",
    "\n",
    "- Comment text: The original comment text. \n",
    "\n",
    "- Doc2Vec vectors: For each subreddit, a Doc2Vec embedding model is trained using all of the sub's text samples. Then all comments in the sub are converted into Doc2Vec vectors to be used as model features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ./data_scored/comment_sample_aww_scored.csv\n",
      "reading ./data_scored/comment_sample_funny_scored.csv\n",
      "reading ./data_scored/comment_sample_todayilearned_scored.csv\n",
      "reading ./data_scored/comment_sample_askreddit_scored.csv\n",
      "reading ./data_scored/comment_sample_photography_scored.csv\n",
      "reading ./data_scored/comment_sample_gaming_scored.csv\n",
      "reading ./data_scored/comment_sample_videos_scored.csv\n",
      "reading ./data_scored/comment_sample_science_scored.csv\n",
      "reading ./data_scored/comment_sample_politics_scored.csv\n",
      "reading ./data_scored/comment_sample_politicaldiscussion_scored.csv\n",
      "reading ./data_scored/comment_sample_conservative_scored.csv\n",
      "reading ./data_scored/comment_sample_the_Donald_scored.csv\n",
      "\n",
      "Total comment samples read: 3251370\n"
     ]
    }
   ],
   "source": [
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# ---\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "# source data folder \n",
    "srcdir = './data_scored/' # dir of comment data with PCA scores added\n",
    "destdir = './data_for_models/'\n",
    "\n",
    "# the subreddits I'll be analyzing\n",
    "sub2use = ['aww', 'funny', 'todayilearned','askreddit',\n",
    "           'photography', 'gaming', 'videos', 'science',\n",
    "           'politics', 'politicaldiscussion',             \n",
    "           'conservative', 'the_Donald']\n",
    "\n",
    "# load all labelled CSVs\n",
    "dfs = []\n",
    "for subname in sub2use:\n",
    "    pathname = srcdir+'comment_sample_'+subname+'_scored.csv'\n",
    "    print('reading',pathname)\n",
    "    tdf = pd.read_csv(pathname)\n",
    "    dfs.append(tdf)\n",
    "\n",
    "# combine all subreddit datasets into one  \n",
    "df = pd.concat(dfs)\n",
    "\n",
    "# remove any deleted or removed comments \n",
    "df = df[(df.text!='[deleted]') & (df.text!='[removed]')]\n",
    "\n",
    "# drop samples with NaNs\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# drop duplicates\n",
    "df = df.drop_duplicates(subset='comment_ID')\n",
    "\n",
    "# reformat parent ids to match comment ids\n",
    "df.parent_ID = df.parent_ID.str.replace('t1_','')\n",
    "df.parent_ID = df.parent_ID.str.replace('t3_','')\n",
    "\n",
    "print('\\nTotal comment samples read:',df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the text\n",
    "\n",
    "Clean up the text:\n",
    "- Remove non-alphanumeric characters\n",
    "- Remove stop words\n",
    "- Make lowercase\n",
    "- Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "# function to prepare text for NLP analysis\n",
    "def process_comment_text(comments, \n",
    "                         stemmer=None, \n",
    "                         regexstr=None, lowercase=True,\n",
    "                         removestop=False,\n",
    "                         verbose=True):\n",
    "    \"\"\"Helper function to pre-process text.\n",
    "        Combines several preprocessing steps: lowercase, \n",
    "            remove stop, regex text cleaning, stemming\"\"\"\n",
    "    \n",
    "    if type(stemmer) == str:\n",
    "        if stemmer.lower() == 'porter':\n",
    "            stemmer = PorterStemmer()\n",
    "        elif stemmer.lower() == 'snowball':\n",
    "            stemmer = SnowballStemmer(language='english')\n",
    "        else:\n",
    "            stemmer = None\n",
    "            \n",
    "    processed = comments\n",
    "    \n",
    "    # make text lowercase\n",
    "    if lowercase == True:\n",
    "        if verbose: print('make text lowercase')\n",
    "        processed = processed.str.lower()\n",
    "        \n",
    "    # remove stop words\n",
    "    # NOTE: stop words w/ capitals not removed!\n",
    "    if removestop == True:\n",
    "        if verbose: print('remove stop words')\n",
    "        stopwords = sw.words(\"english\")\n",
    "        processed = processed.map(lambda text: ' '.join([word for word in text.split() if word not in stopwords]))\n",
    "        \n",
    "    # apply regex expression\n",
    "    if regexstr is not None:\n",
    "        if verbose: print('apply regex expression')\n",
    "        regex = re.compile(regexstr) \n",
    "        processed = processed.str.replace(regex,' ')\n",
    "        \n",
    "    # stemming\n",
    "    # NOTE: stemming makes all lowercase\n",
    "    if stemmer is not None:\n",
    "        if verbose: print('stemming')\n",
    "        processed = processed.map(lambda x: ' '.join([stemmer.stem(y) for y in x.split(' ')]))\n",
    "        \n",
    "    if verbose: print('done')\n",
    "         \n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the text features file.\n",
    "\n",
    "- Do minimal text processing \n",
    "- File includes comment metadata and toxicity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_features(df, subname, destdir):\n",
    "    print('generating text features')\n",
    "    # specify parameters for text prep\n",
    "    processkwargs = {\n",
    "        'stemmer': None, #'snowball', # snowball stemmer\n",
    "        'regexstr':'[^a-zA-Z0-9\\s]', # remove all but alphanumeric chars\n",
    "        'lowercase':True, # make lowercase\n",
    "        'removestop':False # remove stop words \n",
    "                    }\n",
    "    df['text'] = process_comment_text(df['text'], **processkwargs, verbose=True)\n",
    "    df.to_csv(destdir+'features_text_'+subname+'.csv',index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Doc2Vec features file\n",
    "\n",
    "- Create a Doc2Vec word embedding for each subreddit.\n",
    "- Convert comment text to 100 Doc2Vec vectors to use as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "from nltk.tokenize import word_tokenize\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "def generate_doc2vec_features(df, subname, destdir):\n",
    "    print('generating doc2vec features')\n",
    "    # specify parameters for text prep\n",
    "    processkwargs = {\n",
    "        'stemmer': None, #'snowball', # snowball stemmer\n",
    "        'regexstr':'[^a-zA-Z0-9\\s]', # remove all but alphanumeric chars\n",
    "        'lowercase':False, # make lowercase\n",
    "        'removestop':False # remove stop words \n",
    "                    }\n",
    "    d2v_text = process_comment_text(df['text'], **processkwargs, verbose=True)\n",
    "    \n",
    "    # convert text to tagged document format\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(d2v_text)]\n",
    "\n",
    "    print('  Creating new model...')\n",
    "\n",
    "    # some relevant doc2vec model parameters\n",
    "    ndims = 100 # number of embedding features\n",
    "    windowsize = 2 # word distance window size\n",
    "    \n",
    "    # train doc2vec model\n",
    "    model_d2v = Doc2Vec(documents, vector_size=ndims, window=windowsize, \n",
    "                    min_count=1, workers=4)\n",
    "    \n",
    "    # create array to contain the embedding vectors\n",
    "    vec = np.zeros([df.shape[0], model_d2v.vector_size])\n",
    "    print('  vector dataframe shape:',vec.shape)\n",
    "\n",
    "    print('  vectorizing comments...')\n",
    "    t0 = time()\n",
    "\n",
    "    # convert the text into embedding vectors\n",
    "    for i,text in enumerate(d2v_text):\n",
    "        vec[i,:] = model_d2v.infer_vector(text.split())\n",
    "        \n",
    "    # convert w2vec vetor features into a dataframe\n",
    "    d2v_df = pd.DataFrame(vec, columns=['dv_'+str(i) for i in range(vec.shape[1])])\n",
    "\n",
    "    # save only features\n",
    "    d2v_df.to_csv(destdir+'features_doc2vec_'+subname+'.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate separate feature files for each subreddit sampled from:\n",
    "\n",
    "- text features (plus meta info and generated features)\n",
    "- word2vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "processing sub aww\n",
      "generating text features\n",
      "make text lowercase\n",
      "apply regex expression\n",
      "done\n",
      "generating doc2vec features\n",
      "apply regex expression\n",
      "done\n",
      "  Creating new model...\n",
      "  vector dataframe shape: (217359, 100)\n",
      "  vectorizing comments...\n",
      "\n",
      "processing sub funny\n",
      "generating text features\n",
      "make text lowercase\n",
      "apply regex expression\n",
      "done\n",
      "generating doc2vec features\n",
      "apply regex expression\n",
      "done\n",
      "  Creating new model...\n",
      "  vector dataframe shape: (255422, 100)\n",
      "  vectorizing comments...\n",
      "\n",
      "processing sub todayilearned\n",
      "generating text features\n",
      "make text lowercase\n",
      "apply regex expression\n",
      "done\n",
      "generating doc2vec features\n",
      "apply regex expression\n",
      "done\n",
      "  Creating new model...\n",
      "  vector dataframe shape: (254687, 100)\n",
      "  vectorizing comments...\n",
      "\n",
      "processing sub askreddit\n",
      "generating text features\n",
      "make text lowercase\n",
      "apply regex expression\n",
      "done\n",
      "generating doc2vec features\n",
      "apply regex expression\n",
      "done\n",
      "  Creating new model...\n",
      "  vector dataframe shape: (198972, 100)\n",
      "  vectorizing comments...\n",
      "\n",
      "processing sub photography\n",
      "generating text features\n",
      "make text lowercase\n",
      "apply regex expression\n",
      "done\n",
      "generating doc2vec features\n",
      "apply regex expression\n",
      "done\n",
      "  Creating new model...\n",
      "  vector dataframe shape: (143707, 100)\n",
      "  vectorizing comments...\n",
      "\n",
      "processing sub gaming\n",
      "generating text features\n",
      "make text lowercase\n",
      "apply regex expression\n",
      "done\n",
      "generating doc2vec features\n",
      "apply regex expression\n",
      "done\n",
      "  Creating new model...\n",
      "  vector dataframe shape: (389947, 100)\n",
      "  vectorizing comments...\n",
      "\n",
      "processing sub videos\n",
      "generating text features\n",
      "make text lowercase\n",
      "apply regex expression\n",
      "done\n",
      "generating doc2vec features\n",
      "apply regex expression\n",
      "done\n",
      "  Creating new model...\n",
      "  vector dataframe shape: (413763, 100)\n",
      "  vectorizing comments...\n",
      "\n",
      "processing sub science\n",
      "generating text features\n",
      "make text lowercase\n",
      "apply regex expression\n",
      "done\n",
      "generating doc2vec features\n",
      "apply regex expression\n",
      "done\n",
      "  Creating new model...\n",
      "  vector dataframe shape: (152387, 100)\n",
      "  vectorizing comments...\n",
      "\n",
      "processing sub politics\n",
      "generating text features\n",
      "make text lowercase\n",
      "apply regex expression\n",
      "done\n",
      "generating doc2vec features\n",
      "apply regex expression\n",
      "done\n",
      "  Creating new model...\n",
      "  vector dataframe shape: (361389, 100)\n",
      "  vectorizing comments...\n",
      "\n",
      "processing sub politicaldiscussion\n",
      "generating text features\n",
      "make text lowercase\n",
      "apply regex expression\n",
      "done\n",
      "generating doc2vec features\n",
      "apply regex expression\n",
      "done\n",
      "  Creating new model...\n",
      "  vector dataframe shape: (350389, 100)\n",
      "  vectorizing comments...\n",
      "\n",
      "processing sub conservative\n",
      "generating text features\n",
      "make text lowercase\n",
      "apply regex expression\n",
      "done\n",
      "generating doc2vec features\n",
      "apply regex expression\n",
      "done\n",
      "  Creating new model...\n",
      "  vector dataframe shape: (124302, 100)\n",
      "  vectorizing comments...\n",
      "\n",
      "processing sub the_Donald\n",
      "generating text features\n",
      "make text lowercase\n",
      "apply regex expression\n",
      "done\n",
      "generating doc2vec features\n",
      "apply regex expression\n",
      "done\n",
      "  Creating new model...\n",
      "  vector dataframe shape: (389046, 100)\n",
      "  vectorizing comments...\n"
     ]
    }
   ],
   "source": [
    "# generate separate feature files for each subreddit sampled from\n",
    "sublist = df['sub_name'].unique()\n",
    "\n",
    "for subname in sublist:\n",
    "    print('\\nprocessing sub',subname)\n",
    "    subdf = df[df['sub_name']==subname]\n",
    "    generate_text_features(subdf, subname, destdir)\n",
    "    generate_doc2vec_features(subdf, subname, destdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
