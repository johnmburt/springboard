{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating \"Reply-to\" data for reply comments\n",
    "\n",
    "### John Burt\n",
    "\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "The social media site Reddit is divided into many different communities called subreddits (subs). Each sub covers a specific topic or theme and tends to have regular users posting comments. The dataset I'm working with consists of comments to posts from several different subs.\n",
    "\n",
    "###  Notebook purpose:\n",
    "\n",
    "This notebook contains code to add \"replyto\" data columns to the main dataset, based on the parent ID column in each sample. The process wasn't a simple as I thought, and it takes a long time to complete:\n",
    "\n",
    "- Convert ID text labels to category numbers: make ID_n cols for post_ID, parent_ID, comment_ID\n",
    "- Find all unique parent IDs.\n",
    "- Remove parent IDs that are actually the post ID (top level comments).\n",
    "- Iterate through each parent ID:\n",
    "    - Get parent comment row.\n",
    "    - Find all reply comments with that parent ID.\n",
    "    - Set replyto info for each reply comment using parent comment \n",
    "    \n",
    "I've also included a bit of code from an analysis of computation speed of different methods I tried. Hopefully this will be helpful for future projects requiring non-trivial searchs through data matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "The comment data used in this analysis was [acquired using PRAW](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_collect_comments_v1.ipynb) from 12 subs. 8 of the subs are non-political, and 4 are political in nature. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total comment samples read: 3251323\n"
     ]
    }
   ],
   "source": [
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# ---\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "\n",
    "# source data folder \n",
    "srcdir = './data_labeled/'\n",
    "\n",
    "# the subreddits I'll be analyzing\n",
    "sub2use = ['aww', 'funny', 'todayilearned','askreddit',\n",
    "           'photography', 'gaming', 'videos', 'science',\n",
    "           'politics', 'politicaldiscussion',             \n",
    "           'conservative', 'the_Donald']\n",
    "\n",
    "# load all labelled CSVs\n",
    "dfs = []\n",
    "for subname in sub2use:\n",
    "    pathname = srcdir+'comment_sample_'+subname+'_labeled.csv'\n",
    "#     print('reading',pathname)\n",
    "    tdf = pd.read_csv(pathname)\n",
    "    dfs.append(tdf)\n",
    "\n",
    "# combine all subreddit datasets into one  \n",
    "df = pd.concat(dfs).drop_duplicates()\n",
    "\n",
    "# remove any deleted or removed comments \n",
    "df = df[(df.text!='[deleted]') & (df.text!='[removed]')]\n",
    "\n",
    "# drop samples with NaNs\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# drop duplicates\n",
    "df = df.drop_duplicates(subset='comment_ID')\n",
    "\n",
    "# reformat parent ids to match comment ids\n",
    "df.parent_ID = df.parent_ID.str.replace('t1_','')\n",
    "df.parent_ID = df.parent_ID.str.replace('t3_','')\n",
    "\n",
    "print('\\nTotal comment samples read:',df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create an ID to category number mapping\n",
    "\n",
    "I do this to help speed up the reply comment search below.\n",
    "\n",
    "Post IDs are set to a category num of -1 to make it easier to remove them from the parent ID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Note: to get ID strings from ID numbers, invert the dict:\n",
    "# invert a dict: {v:k  for k,v in my_dict.items()}\n",
    "\n",
    "# create a dict that makes all post IDs = -1\n",
    "pid = list(df['post_ID'].unique())\n",
    "id_dict = OrderedDict(zip(pid,[-1]*len(pid)))\n",
    "\n",
    "# create list of parent IDs with post IDs (ie top level comments) removed\n",
    "spo = set(pid)\n",
    "spa = set(df['parent_ID'].values)\n",
    "parentids = list(spa.difference(spo))\n",
    "\n",
    "# create a list containing only unique comment IDs\n",
    "allcomids = np.unique(parentids + list(df['comment_ID'].values))\n",
    "\n",
    "# create a dict containing all comment ids, with \n",
    "comiddict = OrderedDict(zip(allcomids,range(len(allcomids))))\n",
    "\n",
    "# combine the post dict and the all comment dict\n",
    "id_dict.update(comiddict)\n",
    "\n",
    "# convert parent IDs to category values\n",
    "df['parent_ID_n'] = [id_dict[x] for x in df['parent_ID']]\n",
    "\n",
    "# convert comment IDs to category values\n",
    "df['comment_ID_n'] = [id_dict[x] for x in df['comment_ID']]\n",
    "                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the new columns, fill them with null data\n",
    "\n",
    "I will load these columns with reply-to data, then add them to the comment dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "numrows = df.shape[0]\n",
    "\n",
    "replyto_id = np.array([object]*numrows)\n",
    "replyto_score = np.array([np.nan]*numrows)\n",
    "replyto_pca_score = np.array([np.nan]*numrows)\n",
    "replyto_text = np.array([object]*numrows)\n",
    "replyto_num_replies = np.array([np.nan]*numrows)\n",
    "replyto_com_karma = np.array([np.nan]*numrows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe containing only comments that are parents to other comments. \n",
    "\n",
    "These are the comments I will iterate through to locate replies to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(649484, 28) (649484,)\n"
     ]
    }
   ],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.set_index('comment_ID_n', inplace=True)\n",
    "\n",
    "par_df = df.loc[np.unique(df['parent_ID_n'][df['parent_ID_n']>=0])]\n",
    "par_df = par_df.drop_duplicates(subset='parent_ID_n')\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "par_df.reset_index(inplace=True)\n",
    "\n",
    "print(par_df.shape, par_df['parent_ID_n'].unique().shape)\n",
    "\n",
    "# parent_ID = df['parent_ID'].values\n",
    "parent_ID_n = df['parent_ID_n'].values\n",
    "# comment_ID = df['comment_ID'].values\n",
    "# comment_ID_n = df['comment_ID_n'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each parent comment, find all replies to it\n",
    "Note: this takes a really long time!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000  10000  15000  20000  25000  30000  35000  40000  45000  50000  55000  60000  65000  70000  75000  80000  85000  90000  95000  100000  105000  110000  115000  120000  125000  130000  135000  140000  145000  150000  155000  160000  165000  170000  175000  180000  185000  190000  195000  200000  205000  210000  215000  220000  225000  230000  235000  240000  245000  250000  255000  260000  265000  270000  275000  280000  285000  290000  295000  300000  305000  310000  315000  320000  325000  330000  335000  340000  345000  350000  355000  360000  365000  370000  375000  380000  385000  390000  395000  400000  405000  410000  415000  420000  425000  430000  435000  440000  445000  450000  455000  460000  465000  470000  475000  480000  485000  490000  495000  500000  505000  510000  515000  520000  525000  530000  535000  540000  545000  550000  555000  560000  565000  570000  575000  580000  585000  590000  595000  600000  605000  610000  615000  620000  625000  630000  635000  640000  645000  "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "# for par_idx, par_idn in enumerate(par_df['parent_ID_n']):\n",
    "for paridx, parent in par_df.iterrows():\n",
    "    idx, = np.where(parent_ID_n==parent['comment_ID_n'])\n",
    "    replyto_id[idx] = parent['comment_ID']\n",
    "    replyto_score[idx] = parent['score']\n",
    "    replyto_pca_score[idx] = parent['pca_score'] \n",
    "    replyto_text[idx] = parent['text']\n",
    "    replyto_num_replies[idx] = parent['num_replies']\n",
    "    replyto_com_karma[idx] = parent['u_comment_karma']\n",
    "    count += 1\n",
    "    if not count % 5000:\n",
    "        print(count,' ',end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3251323, 28) 3251323 1048906\n"
     ]
    }
   ],
   "source": [
    "print(df.shape,len(replyto_score),np.sum(~np.isnan(replyto_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign the reply-to np vectors to columns in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the replyto np vectors to columns in df\n",
    "\n",
    "df['replyto_id'] = replyto_id\n",
    "df['replyto_score'] = replyto_score\n",
    "df['replyto_pca_score'] = replyto_pca_score\n",
    "df['replyto_text'] = replyto_text\n",
    "df['replyto_num_replies'] = replyto_num_replies\n",
    "df['replyto_com_karma'] = replyto_com_karma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the results\n",
    "\n",
    "The algorithm is a bit complicated, it's possible the output is wrong, so I'd better doublecheck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# parent_ID != replyto_id: 0\n",
      "# not same post ID: 0\n",
      "# not same subname: 0\n",
      "\n",
      "Parent index: 527245\n",
      "------------------------\n",
      "Parent comment: Cops are not going to do shit for you, except make the situation worse.\n",
      "Reply: They wouldn't need to since I was in the right. They also wouldnt do shit for the bar/restaurant. This would end in mine, the cops, and teh establishments time being wasted. Probably a ban from going there in the future. \n",
      "\n",
      "But no one is going to jail and that tab isn't being paid by OP.\n",
      "\n",
      "For $500, I will hang out til the cops show up vs fighting a bouncer or waiter or running away on the tab. \n",
      "\n",
      "What would you suggest in this situation if you don't feel that the cops would be any help if it got to that point? Just pay? Run? Physically be confronted by an aggressive bouncer/owner when you try and leave without paying the disputed bill? \n",
      "\n",
      "IF it escalated beyond \"I am not paying this amount.\" to \"Fuck you aren't leaving til you pay and I will physically stop you if you try.\" then I am waiting for the cops if it comes to that. An hour or so of my time is worth saving the $500 and/or getting an actual serious charge for fighting or some shit. \n",
      "\n",
      "Parent index: 1149462\n",
      "------------------------\n",
      "Parent comment: Obamacare covers a fair bit of screening.  Check into it. \n",
      "I went in for a colonoscopy a year ago, all covered. \n",
      "\n",
      "Reply: Unless, like me, you live in a state that didn't expand medicaid. As a result, I can't afford any sort of healthcare coverage.\n",
      "\n",
      "Parent index: 2382047\n",
      "------------------------\n",
      "Parent comment: Yup, because of a single vote, the GOP lost control of the House of Delegates for the whole state.  I'm going to have to look up how they're going to function with no majority. Odd numbers people!  Use them.\n",
      "Reply: Coalitions(maybe all of one party and a couple of line crossers with some benefits)\n",
      "\n",
      "Parent index: 2209848\n",
      "------------------------\n",
      "Parent comment: * Ford pardoned Nixon.\n",
      "* Bush pardoned Caspar Weinberger and all the other Iran-Contra criminals.\n",
      "\n",
      "It'll be interesting to see what Trump's successor does, assuming any of these accusations pan out. My guess is that if Mueller has an ironclad case, Trump will resign so that Pence can pardon him before a Dem takes office in 2020.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reply: State charges... AG of NY aint gonna let this go.\n",
      "\n",
      "Parent index: 667682\n",
      "------------------------\n",
      "Parent comment: As long as you dont *have* to buy that stuff to enjoy the game who cares?\n",
      "Reply: As long as the game is built to incentivize microtransactions, it's bad for everyone, regardless of whether you buy them. It also means no support for single player.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do some sanity checking to make sure this is right\n",
    "\n",
    "# select reply comments\n",
    "reply_df = df[~df['replyto_score'].isnull()]\n",
    "\n",
    "# every comment with replyto data: parent_ID should equal replyto_id\n",
    "print('# parent_ID != replyto_id:',\n",
    "      (reply_df['parent_ID'] != reply_df['replyto_id']).sum())\n",
    "\n",
    "# verify that parent_ID_n contents == replyto_id: \n",
    "numsamps = 1000 # number of samples to test\n",
    "test_post = []\n",
    "test_subname = []\n",
    "for comidx, com in reply_df.sample(numsamps).iterrows():\n",
    "    idx, = np.where(comment_ID==com['replyto_id'])  \n",
    "    idx = idx[0]\n",
    "    #  - same post?\n",
    "    test_post.append(df['post_ID'].iloc[idx]!=com['post_ID'])\n",
    "    #  - same sub?\n",
    "    test_subname.append(df['sub_name'].iloc[idx]!=com['sub_name'])\n",
    "\n",
    "print('# not same post ID:',np.sum(test_post))    \n",
    "print('# not same subname:',np.sum(test_subname))   \n",
    "print()\n",
    "    \n",
    "#  - reply text makes sense?\n",
    "for comidx, com in reply_df.sample(5).iterrows():\n",
    "    idx, = np.where(comment_ID==com['replyto_id'])  \n",
    "    idx = idx[0]\n",
    "    print('Parent index:',idx)\n",
    "    print('------------------------')\n",
    "    print('Parent comment:',df['text'].iloc[idx])\n",
    "    print('Reply:',com['text'])\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved reply data to ./data_labeled/comment_sample_all_plus_replies.csv\n"
     ]
    }
   ],
   "source": [
    "# save the reply-enhanced df\n",
    "\n",
    "if True:\n",
    "    replypath = srcdir+'comment_sample_all_plus_replies.csv'\n",
    "    df.to_csv(replypath,index=False)\n",
    "    print('saved reply data to',replypath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeit tests of different methods for searching for a value in a column\n",
    "\n",
    "I've left this here for future reference.\n",
    "\n",
    "#### idx = np.where(paridn==id)\n",
    "- 532 ms ± 5.27 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "#### x = paridn[paridn == id]\n",
    "- 539 ms ± 6.65 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "#### x = df[df['parent_ID_n'] == id]['parent_ID_n']\n",
    "- 819 ms ± 3.49 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "#### idx = (np.abs(paridn - id)).argmin()\n",
    "- 2.91 s ± 152 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-06f26be8667c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#     idx = find_nearest(paridn, id)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#     idx = (np.abs(paridn - id)).argmin()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparidn\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;31m#     print(id,idx.shape,list(idx))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0midxlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    return (np.abs(array - value)).argmin()\n",
    "\n",
    "paridnlist = df['parent_ID_n'].unique()\n",
    "\n",
    "# paridn = df['parent_ID_n'].values\n",
    "paridn = np.asarray(df['parent_ID_n'].values)\n",
    "\n",
    "idxlist = []\n",
    "x = 0\n",
    "for id in df['parent_ID_n'].unique():\n",
    "#     print(df[df['parent_ID_n'] == id]['parent_ID_n'],end='')\n",
    "#     x = df[df['parent_ID_n'] == id]['parent_ID_n']\n",
    "#     x = paridn[paridn == id]\n",
    "#     idx = find_nearest(paridn, id)\n",
    "#     idx = (np.abs(paridn - id)).argmin()\n",
    "    idx, = np.where(paridn==id)\n",
    "#     print(id,idx.shape,list(idx))\n",
    "    idxlist.extend(list(idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old method, extremely slow:\n",
    "\n",
    "Left for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# with replies = 1159680\n",
      "5000  "
     ]
    }
   ],
   "source": [
    "# set replyto cols only for replies to negative comments\n",
    "# df_neg = df[df.score<0]\n",
    "# for index, comment in df_neg.iterrows():\n",
    "df_withreply = df[df['num_replies']>0]\n",
    "print('# with replies =',df_withreply.shape[0])\n",
    "count=0\n",
    "for index, comment in df_withreply.iterrows():\n",
    "#     df['replyto_id'].loc[comment['comment_ID_n']] = comment['comment_ID']\n",
    "#     df['replyto_score'].loc[comment['comment_ID_n']] = comment['score']\n",
    "#     df['replyto_pca_score'].loc[comment['comment_ID_n']] = comment['pca_score']\n",
    "#     df['replyto_text'].loc[comment['comment_ID_n']] = comment['text']\n",
    "#     break\n",
    "    try:\n",
    "        df.loc[comment['comment_ID_n']]['replyto_id'] = comment['comment_ID']\n",
    "        df.loc[comment['comment_ID_n']]['replyto_score'] = comment['score']\n",
    "        df.loc[comment['comment_ID_n']]['replyto_pca_score'] = comment['pca_score']\n",
    "        df.loc[comment['comment_ID_n']]['replyto_text'] = comment['text']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#     replies = df['parent_ID']==comment['comment_ID']\n",
    "#     if replies.any():\n",
    "# #         print(comment['comment_ID'],comment['score'])\n",
    "#         df['replyto_id'][replies] = comment['comment_ID']\n",
    "#         df['replyto_score'][replies] = comment['score']\n",
    "#         df['replyto_pca_score'][replies] = comment['pca_score']\n",
    "#         df['replyto_text'][replies] = comment['text']\n",
    "    count += 1\n",
    "    if not count % 5000: print(count,' ',end='')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
