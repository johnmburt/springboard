{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit toxic comment classifier: <br />Multinomial Naive Bayes\n",
    "## Hyperparameter tuning with hyperopt\n",
    "\n",
    "### John Burt\n",
    "\n",
    "[To hide code cells, view this in nbviewer](https://nbviewer.jupyter.org/github/johnmburt/springboard/blob/master/capstone_1/reddit_toxicity_detection_model_MNB_v1.ipynb) \n",
    "\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "The goal of my first Capstone project is to develop a toxic comment classifier. This notebook will implement a Multinomial Naive Bayes classifier and tune hyperparameters using the hyperopt Baysian hyperparameter optimization package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# ---\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data.\n",
    "\n",
    "The comment data used in this analysis was prepared in three stages:\n",
    "\n",
    "- [acquired using Reddit Python API PRAW](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_collect_comments_v1.ipynb) from 12 subs. 8 of the subs are non-political, and 4 are political in nature. Models are trained on data for only one subreddit at a time, so that they are specialized to that subreddit.\n",
    "\n",
    "\n",
    "- The raw comment metadata was [processed using PCA to produce a single toxicity score](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_generate_PCA_score_v2.ipynb) based on the votes and number of replies. Toxicity score was calculated and normalized within each subreddit and then ranged between -5 and +5 to create a toxicity score comparable between subs. The toxicity score was then thresholded to generate binary \"toxic\" vs. \"not toxic\" labels for supervised model training. The threshold applied was: score <= -1 = \"toxic\", otherwise \"not toxic\". \n",
    "\n",
    "\n",
    "- [Features for training the models were generated](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_comment_create_model_features_v1.ipynb) and saved to two sample aligned feature files for each subreddit. These files are used by the models for input.\n",
    "\n",
    "**Note** that this is a highly unbalanced dataset, with usually less than 10% of comments labelled \"toxic\". For proper training, most models will need to be trained with balanced data, which I achieve by up-sampling the less representative \"toxic\" category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_data(subnames, srcdir, toxic_thresh=-1):\n",
    "    \"\"\"Load and prep the feature data from two matched data files\"\"\"\n",
    "    \n",
    "    # load all data csvs for listed subs into dataframes \n",
    "    base_dfs = []\n",
    "    numeric_dfs = []\n",
    "    for sub in subnames:\n",
    "        base_dfs.append(pd.read_csv(srcdir+'features_text_'+sub+'.csv'))\n",
    "        numeric_dfs.append(pd.read_csv(srcdir+'features_doc2vec_'+sub+'.csv'))\n",
    "        \n",
    "    # concat all sub dfs into one for each data type\n",
    "    base_df = pd.concat(base_dfs, ignore_index=True)\n",
    "    numeric_df = pd.concat(numeric_dfs, ignore_index=True)\n",
    "    \n",
    "    # add numeric metadata features from base df to numeric df\n",
    "    numeric_df['u_comment_karma'] = base_df['u_comment_karma']\n",
    "\n",
    "    # return base df (text and all comment metadata), numeric features, training label\n",
    "    return base_df['text'], numeric_df, np.where(base_df['pca_score']>thresh,0,1)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing function\n",
    "\n",
    "This function prepares text data for training. For most models, the text will be processed further at training time, but pre-processing can save time when training is iterated multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "# function to prepare text for NLP analysis\n",
    "def process_comment_text(comments, \n",
    "                         stemmer=None, \n",
    "                         regexstr=None, lowercase=True,\n",
    "                         removestop=False,\n",
    "                         verbose=True):\n",
    "    \"\"\"Helper function to pre-process text.\n",
    "        Combines several preprocessing steps: lowercase, \n",
    "            remove stop, regex text cleaning, stemming\"\"\"\n",
    "    \n",
    "    if type(stemmer) == str:\n",
    "        if stemmer.lower() == 'porter':\n",
    "            stemmer = PorterStemmer()\n",
    "        elif stemmer.lower() == 'snowball':\n",
    "            stemmer = SnowballStemmer(language='english')\n",
    "        else:\n",
    "            stemmer = None\n",
    "            \n",
    "    processed = comments\n",
    "    \n",
    "    # make text lowercase\n",
    "    if lowercase == True:\n",
    "        if verbose: print('make text lowercase')\n",
    "        processed = processed.str.lower()\n",
    "        \n",
    "    # remove stop words\n",
    "    # NOTE: stop words w/ capitals not removed!\n",
    "    if removestop == True:\n",
    "        if verbose: print('remove stop words')\n",
    "        stopwords = sw.words(\"english\")\n",
    "        processed = processed.map(lambda text: ' '.join([word for word in text.split() if word not in stopwords]))\n",
    "        \n",
    "    # apply regex expression\n",
    "    if regexstr is not None:\n",
    "        if verbose: print('apply regex expression')\n",
    "        regex = re.compile(regexstr) \n",
    "        processed = processed.str.replace(regex,' ')\n",
    "        \n",
    "    # stemming\n",
    "    # NOTE: stemming makes all lowercase\n",
    "    if stemmer is not None:\n",
    "        if verbose: print('stemming')\n",
    "        processed = processed.map(lambda x: ' '.join([stemmer.stem(y) for y in x.split(' ')]))\n",
    "        \n",
    "    if verbose: print('done')\n",
    "         \n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance sample frequencies in training samples\n",
    "\n",
    "The classifier may require balancing of sample frequencies between classes for best results. This function will up-sample to the specified number of samples per class.\n",
    "\n",
    "The balance_classes_sparse function does sample balancing with sparse matrices, such as vectorized BOW data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************************************************************************************\n",
    "from scipy.sparse import vstack, hstack\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "def balance_classes_sparse(X, y, samples_per_class=None, verbose=False):\n",
    "    \"\"\"Equalize number of samples so that all classes have equal numbers of samples.\n",
    "    If samples_per_class==None, then upsample (randomly repeat) all classes to the largest class,\n",
    "      Otherwise, set samples for all classes to samples_per_class.\"\"\"\n",
    "    \n",
    "    def get_samples(arr, numsamples):\n",
    "        if arr.shape[0] >= numsamples:\n",
    "            index = np.arange(arr.shape[0])\n",
    "            np.random.shuffle(index)\n",
    "            return arr[index[:numsamples],:]\n",
    "        else:\n",
    "            samples = arr.copy()\n",
    "            numrepeats = int(numsamples / arr.shape[0])\n",
    "            lastsize = numsamples % arr.shape[0]\n",
    "            for i in range(numrepeats-1):\n",
    "                samples = vstack([samples,arr])\n",
    "            if lastsize > 0:\n",
    "                index = np.arange(arr.shape[0])\n",
    "                np.random.shuffle(index)\n",
    "                samples = vstack([samples, arr[index[:lastsize],:]])\n",
    "            return samples   \n",
    "    \n",
    "    if verbose: \n",
    "        print('Balancing class sample frequencies:')\n",
    "        \n",
    "    # all class IDs\n",
    "    classes =  pd.unique(y)\n",
    "    classes = classes[~np.isnan(classes)]\n",
    "    \n",
    "    # get class with max samples\n",
    "    if verbose: \n",
    "        print('\\tOriginal sample frequencies:')\n",
    "    if samples_per_class is None:\n",
    "        samples_per_class = 0\n",
    "        for c in classes:\n",
    "            if verbose: \n",
    "                print('\\t\\tclass:',c,'#samples:',(np.sum(y==c)))\n",
    "            samples_per_class = np.max([samples_per_class, np.sum(y==c)])\n",
    "    if verbose: \n",
    "        print('\\tNew samples_per_class:',samples_per_class)\n",
    "                              \n",
    "    # combine X and y\n",
    "    Xy = csr_matrix(hstack([X, csr_matrix(np.reshape(y, (-1, 1)))]))\n",
    "       \n",
    "    # create a list of samples for each class with equal sample numbers \n",
    "    newdata = None\n",
    "    for c in classes:\n",
    "        if newdata is None:\n",
    "            newdata = get_samples(Xy[y==c,:], samples_per_class)\n",
    "        else:\n",
    "            newdata = vstack([newdata, get_samples(Xy[y==c,:], samples_per_class)])\n",
    "            \n",
    "    print('\\ttotal new samples:',newdata.shape[0])\n",
    "            \n",
    "    return newdata[:,:-1], newdata[:,-1].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define special classifier class and create model pipeline\n",
    "\n",
    "**Custom class with sample balancing:** Most classifiers benefit from using balanced data, and so I made a custom classifier model that does the data balancing via upsampling at the time of fit. I do this because the toxic comment data is very unequal and so a large number of duplicated samples are created during upsampling. I don't want my code to have to process all of that, so the duplication is done after processing and transforming of the original samples in the pipeline.\n",
    "\n",
    "**Creating training data in the pipeline:** The dataset features used for training are mixed, and consists of three sources: comment text, comment metadata, Doc2Vec vectors. These different feature sets are combined in the pipeline using a ColumnTransformer. This allows hyperopt to optimize parameters for the text vectorizer TfidfVectorizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "    \n",
    "# Custom classifier that balances the training data\n",
    "class MultinomialNB_bal(MultinomialNB):\n",
    "    \"\"\"Wrapper class that balances data by upsampling prior to training\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        bal_X, bal_y = balance_classes_sparse(X, y, verbose=False)\n",
    "        super().fit(bal_X, bal_y, **fit_params)\n",
    "        return self\n",
    "    \n",
    "def build_pipeline(classifier):\n",
    "    \"\"\"Create a pipeline to vectorize text,\n",
    "        combine all features, and pass that to classifier\n",
    "    \"\"\"\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[('tfidf', TfidfVectorizer(),'text')],\n",
    "        remainder=\"passthrough\"\n",
    "        )        \n",
    "    return Pipeline([('pre', preprocessor),\n",
    "                     ('clf', classifier)])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log results of optimized model test\n",
    "\n",
    "This function logs the results of a model test to a CSV logfile. Each model notebook logs to the same file so that results can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path\n",
    "from os import path\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from time import time\n",
    "\n",
    "def log_model_results(logpath, modelname, subname, y_test, y_pred):\n",
    "    \"\"\"Write to CSV log file containing results of model train/test runs\"\"\"\n",
    "    # write the header labels\n",
    "    if not os.path.exists(logpath):\n",
    "        labels = (['date','model','sub','num_nontoxic','num_toxic',\n",
    "                   'acc_nontoxic','acc_toxic','accuracy','precision',\n",
    "                   'recall','balanced_acc','F1','roc_auc'])\n",
    "        with open(logpath, 'a', newline='') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "            csvwriter.writerow(labels)\n",
    "            \n",
    "    # create data row\n",
    "    row = [datetime.datetime.now().strftime('%y%m%d_%H%M%S'),\n",
    "          modelname, subname, \n",
    "          (y_test==0).sum(), (y_test==1).sum(),\n",
    "           '%1.3f'%(((y_test==0) & (y_test==y_pred)).sum()/(y_test==0).sum()),\n",
    "           '%1.3f'%(((y_test==1) & (y_test==y_pred)).sum()/(y_test==1).sum()),\n",
    "           '%1.3f'%(np.sum((y_pred==y_test))/y_test.shape[0]),\n",
    "           '%1.3f'%(precision_score(y_test, y_pred)),\n",
    "           '%1.3f'%(recall_score(y_test, y_pred)),\n",
    "           '%1.3f'%(balanced_accuracy_score(y_test, y_pred)),\n",
    "           '%1.3f'%(f1_score(y_test, y_pred)),\n",
    "           '%1.3f'%(roc_auc_score(y_test, y_pred))\n",
    "          ]\n",
    "    # write the data row\n",
    "    with open(logpath, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "        csvwriter.writerow(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization using Baysian methods\n",
    "\n",
    "This script will tune and test model two different subreddit comment datasets: r/gaming and r/politics. I do this because political subs seem to have different voting and comment behavior than other types of sub, so I want to verify that the performance and/or hyperparameters aren't going to be very different.\n",
    "\n",
    "Note: this article provided useful code for pipelines with hyperopt:\n",
    "- [Hyperparameter Tuning with hyperopt in Python](http://steventhornton.ca/blog/hyperparameter-tuning-with-hyperopt-in-python.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "\n",
      "Tuning model using sub gaming\n",
      "  loading feature data\n",
      "    done in 24.046s, X_text.shape, X_numeric.shape, y.shape: (751336,) (751336, 103) (751336,)\n",
      "  preparing text for vectorization\n",
      "    done in 227.141s,\n",
      "  train/test split\n",
      "    done in 1.827s, X_train.shape, X_test.shape: (676202, 102) (75134, 102)\n",
      "  tune model\n",
      "100%|██████████████████████████████████████████████| 100/100 [3:27:05<00:00, 124.26s/it, best loss: 0.4448326386812629]\n",
      "    done in 12425.859s,\n",
      "\n",
      "  Best parameters: {'pre__tfidf__max_df': 0.5338879288102432, 'pre__tfidf__min_df': 2, 'pre__tfidf__ngram_range': (1, 2), 'pre__tfidf__stop_words': None, 'pre__tfidf__sublinear_tf': False, 'pre__tfidf__use_idf': False}\n",
      "\ttotal new samples: 1294656\n",
      "\n",
      "  model performance:\n",
      "    Test set: #nontoxic = 71918  #toxic = 3216\n",
      "    overall accuracy: 0.805\n",
      "    Precision: 0.067\n",
      "    Recall: 0.277\n",
      "    Balanced Accuracy: 0.553\n",
      "    F1 Score: 0.108\n",
      "    ROC AUC: 0.553\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not enough arguments for format string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-c3edf1ab9135>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'    ROC AUC: %1.3f'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n  Total time to optimize model for sub %s = %3.1 min'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: not enough arguments for format string"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from time import time\n",
    "\n",
    "from hyperopt import tpe, hp, fmin, Trials\n",
    "from hyperopt import space_eval\n",
    "\n",
    "# source data folder \n",
    "srcdir = './data_for_models/'\n",
    "\n",
    "# subs to use for this analysis\n",
    "sub2use = ['gaming', 'politics']\n",
    "\n",
    "# apply a threshold to determine toxic vs not toxic\n",
    "thresh = -1\n",
    "\n",
    "# results logfile path\n",
    "logpath = srcdir + 'model_results_log.csv'\n",
    "\n",
    "# name of model\n",
    "modelname = 'MultinomialNB'\n",
    "    \n",
    "# specify parameters for text prep\n",
    "processkwargs = {\n",
    "    'stemmer':'snowball', # snowball stemmer\n",
    "    'regexstr':None, # remove all but alphanumeric chars\n",
    "    'lowercase':False, # make lowercase\n",
    "    'removestop':False, # don't remove stop words \n",
    "    'verbose':False\n",
    "                }\n",
    "\n",
    "# define model defaults for TF-IDF vectorizer.\n",
    "tfidfargs = {\n",
    "    \"analyzer\":'word', \n",
    "    \"max_features\" : 10000,\n",
    "    \"max_df\" : 0.5, # Filters out terms that occur in more than half of the docs (max_df=0.5)\n",
    "    \"min_df\" : 5, # Filters out terms that occur in only one document (min_df=2).\n",
    "    \"ngram_range\":(1, 3), # unigrams\n",
    "    \"stop_words\" : 'english',   #None, #\"english\", # Strips out “stop words”\n",
    "    \"use_idf\" : True\n",
    "    }\n",
    "\n",
    "# hyperopt objective function\n",
    "def objective(params):\n",
    "    global X_train, y_train\n",
    "    clf = build_pipeline(MultinomialNB_bal)\n",
    "    clf.set_params(**params)\n",
    "    # score is cross-validated \n",
    "    score = cross_val_score(clf, X_train, y_train, \n",
    "                            scoring='balanced_accuracy',n_jobs=3).mean()\n",
    "    # return 1-score because hyperopt uses minimization  \n",
    "    return 1-score\n",
    "   \n",
    "# hyperopt parameter space\n",
    "paramspace = {\n",
    "    'pre__tfidf__stop_words': hp.choice('stop_words', ['english', None]),\n",
    "    'pre__tfidf__use_idf': hp.choice('use_idf', [True, False]),\n",
    "    'pre__tfidf__sublinear_tf': hp.choice('sublinear_tf', [True, False]),\n",
    "    'pre__tfidf__min_df': 1+hp.randint('min_df', 5),\n",
    "    'pre__tfidf__max_df': hp.uniform('max_df', 0.5, 1.0),\n",
    "    'pre__tfidf__ngram_range': hp.choice('ngram_range', [(1, 1),(1, 2), (1, 3)])\n",
    "    }\n",
    "\n",
    "# loop through to tune model with comments from each specified subreddit \n",
    "for subname in sub2use:\n",
    "    t0 = tstart = time()\n",
    "\n",
    "    print('------------------------------------------------------')\n",
    "    print('\\nTuning model %s using sub %s'%(modelname,subname))\n",
    "    \n",
    "    print('  loading feature data')\n",
    "    t0 = time()\n",
    "    X_text, X_numeric, y = load_feature_data(sub2use, srcdir, toxic_thresh=thresh)\n",
    "    print('    done in %0.3fs,'%(time() - t0),'X_text.shape, X_numeric.shape, y.shape:',X_text.shape, X_numeric.shape, y.shape)\n",
    "    \n",
    "    print('  preparing text for vectorization')\n",
    "    X_text = process_comment_text(X_text, **processkwargs)\n",
    "    print('    done in %0.3fs,'%(time() - t0))\n",
    "    \n",
    "    # combine X data into df so I can do train_test_split now, before the text is further processed\n",
    "    dvcols = [s for s in X_numeric.columns if 'dv_' in s ]\n",
    "    cols2use = dvcols + ['u_comment_karma']\n",
    "    X_df = pd.concat([pd.DataFrame({'text':X_text}),pd.DataFrame(X_numeric[cols2use])],axis=1,ignore_index=True)   \n",
    "    X_df.columns = ['text'] + cols2use\n",
    "    \n",
    "    # numeric features must be >= 0 \n",
    "    X_df[cols2use] = X_df[cols2use] - X_df[cols2use].min().min()\n",
    "    \n",
    "    # Split into test and training data\n",
    "    t0 = time()\n",
    "    print('  train/test split')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_df, y,  test_size=0.1, random_state=42)\n",
    "    print('    done in %0.3fs,'%(time() - t0),'X_train.shape, X_test.shape:', X_train.shape, X_test.shape)\n",
    "   \n",
    "    # hyperparameter tuning:\n",
    "    # The Trials object will store details of each iteration\n",
    "    trials = Trials()\n",
    "\n",
    "    # Run the hyperparameter search using the tpe algorithm\n",
    "    t0 = time()\n",
    "    print('  tune model')\n",
    "    best = fmin(fn=objective, space=paramspace, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "    print('    done in %0.3fs,'%(time() - t0))\n",
    "\n",
    "    # Get the values of the optimal parameters\n",
    "    best_params = space_eval(paramspace, best)\n",
    "    print('\\n  Best parameters:',best_params)   \n",
    "\n",
    "    # build model pipeline\n",
    "    clf = build_pipeline(MultinomialNB_bal)\n",
    "\n",
    "    # set model with the optimal hyperparamters\n",
    "    clf.set_params(**best_params)\n",
    "\n",
    "    # fit the classifier with training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # y_out = clf.predict_proba(X_test)[:,1]\n",
    "    y_out = clf.predict(X_test)\n",
    "    y_pred = (np.where(y_out>.5,1,0))\n",
    "    \n",
    "    # log the results\n",
    "    log_model_results(logpath, modelname, subname, y_test, y_pred)\n",
    "\n",
    "    print('\\n  model performance:')\n",
    "    print('    Test set: #nontoxic =', (y_test==0).sum(), ' #toxic =', (y_test==1).sum())\n",
    "    print('    overall accuracy: %1.3f'%(np.sum((y_pred==y_test))/y_test.shape[0]))\n",
    "    print('    Precision: %1.3f'%(precision_score(y_test, y_pred)))\n",
    "    print('    Recall: %1.3f'%(recall_score(y_test, y_pred)))\n",
    "    print('    Balanced Accuracy: %1.3f'%(balanced_accuracy_score(y_test, y_pred)))\n",
    "    print('    F1 Score: %1.3f'%(f1_score(y_test, y_pred)))\n",
    "    print('    ROC AUC: %1.3f'%(roc_auc_score(y_test, y_pred)))    \n",
    "    \n",
    "    print('\\n  Total time to optimize model for sub %s = %3.1f min'%(subname, (time() - tstart)/60))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# testing hyperopt to be sure it will work with given config\n",
    "# best = fmin(fn=objective, space=paramspace, algo=tpe.suggest, max_evals=100, trials=trials)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
