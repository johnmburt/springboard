{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit toxic comment classifier: <br />Multinomial Naive Bayes\n",
    "## K folds cross-validation over all subs\n",
    "\n",
    "### John Burt\n",
    "\n",
    "[To hide code cells, view this in nbviewer](https://nbviewer.jupyter.org/github/johnmburt/springboard/blob/master/capstone_1/reddit_toxicity_detection_model_MNB_v1.ipynb) \n",
    "\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "The goal of my first Capstone project is to develop a toxic comment classifier. This notebook will train a Multinomial Naive Bayes classifier to detect toxic Reddit comments, using tuned hyperparameters, and test it with K folds cross-validation. The script will train and test all subreddit datasets in turn and will report performance statistics.\n",
    "\n",
    "### Load the data.\n",
    "\n",
    "The comment data used in this analysis was prepared in three stages:\n",
    "\n",
    "- [acquired using Reddit Python API PRAW](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_collect_comments_v1.ipynb) from 12 subs. 8 of the subs are non-political, and 4 are political in nature. Models are trained on data for only one subreddit at a time, so that they are specialized to that subreddit.\n",
    "\n",
    "\n",
    "- The raw comment metadata was [processed using PCA to produce a single toxicity score](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_generate_PCA_score_v2.ipynb) based on the votes and number of replies. Toxicity score was calculated and normalized within each subreddit and then ranged between -5 and +5 to create a toxicity score comparable between subs. The toxicity score was then thresholded to generate binary \"toxic\" vs. \"not toxic\" labels for supervised model training. The threshold applied was: score <= -1 = \"toxic\", otherwise \"not toxic\". \n",
    "\n",
    "\n",
    "- [Features for training the models were generated](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_comment_create_model_features_v1.ipynb) and saved to two sample aligned feature files for each subreddit. These files are used by the models for input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'capstone1_helper' from 'C:\\\\Users\\\\john\\\\notebooks\\\\reddit\\\\capstone1_helper.py'>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# ---\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "# import helper functions\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "import capstone1_helper\n",
    "import importlib\n",
    "importlib.reload(capstone1_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# cross-validation of classifier model with text string data X, category labels in y\n",
    "# ** NOTE: X and y must be passed as pandas objects\n",
    "def cross_validate_classifier(clf, X, y, logpath, modelname, subname, balance=True):\n",
    "    \"\"\"Set up kfold to generate several train-test sets, \n",
    "        then train and test\"\"\" \n",
    "        \n",
    "    kf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    i = 1\n",
    "    accuracy = []\n",
    "    print('    ',end='')\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "        print('*',end='')\n",
    "\n",
    "        # balance label categories by upsampling\n",
    "        if balance:\n",
    "            X_train, y_train = capstone1_helper.balance_classes_sparse(\n",
    "                X[train_index,:], y[train_index], verbose=False)\n",
    "        else:\n",
    "            X_train = X[train_index,:]\n",
    "            y_train = y[train_index]\n",
    "            \n",
    "        # extract test set for this fold\n",
    "        X_test = X[test_index,:]\n",
    "        y_test = y[test_index]\n",
    "\n",
    "        # train the model\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # generate predictions for test data\n",
    "        y_est = clf.predict(X[test_index,:])\n",
    "        y_pred = (np.where(y_est>.5,1,0))\n",
    "\n",
    "        # log the results\n",
    "        capstone1_helper.log_model_results(logpath, modelname, subname, y_test, y_pred)\n",
    "        \n",
    "        # store the balanced accuracy stat\n",
    "        accuracy.append(balanced_accuracy_score(y_test, y_pred))\n",
    "        i += 1\n",
    "\n",
    "    print(\"\\n    Mean balanced accuracy over %d folds = %2.1f%%\"%(\n",
    "        len(accuracy), np.mean(accuracy)*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test all subs with optimized parameters\n",
    "\n",
    "This script will validation test a model for all subreddit datasets, using hyperparameters optimized with hyperopt in a previous notebook. The model will be K folds cross-validated with data for each subreddit, and the results will be saved to a common logfile so that cross-model comparisons can be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------\n",
      "Testing model MultinomialNB using sub aww\n",
      "  cross-validating\n",
      "    ***\n",
      "    Mean balanced accuracy over 3 folds = 71.6%\n",
      "    done in 0.2 min,\n",
      "\n",
      "------------------------------------------------------\n",
      "Testing model MultinomialNB using sub funny\n",
      "  cross-validating\n",
      "    ***\n",
      "    Mean balanced accuracy over 3 folds = 65.3%\n",
      "    done in 0.2 min,\n",
      "\n",
      "------------------------------------------------------\n",
      "Testing model MultinomialNB using sub todayilearned\n",
      "  cross-validating\n",
      "    ***\n",
      "    Mean balanced accuracy over 3 folds = 73.3%\n",
      "    done in 0.2 min,\n",
      "\n",
      "------------------------------------------------------\n",
      "Testing model MultinomialNB using sub askreddit\n",
      "  cross-validating\n",
      "    ***\n",
      "    Mean balanced accuracy over 3 folds = 58.6%\n",
      "    done in 0.2 min,\n",
      "\n",
      "------------------------------------------------------\n",
      "Testing model MultinomialNB using sub photography\n",
      "  cross-validating\n",
      "    ***\n",
      "    Mean balanced accuracy over 3 folds = 57.1%\n",
      "    done in 0.1 min,\n",
      "\n",
      "------------------------------------------------------\n",
      "Testing model MultinomialNB using sub gaming\n",
      "  cross-validating\n",
      "    ***\n",
      "    Mean balanced accuracy over 3 folds = 56.3%\n",
      "    done in 0.4 min,\n",
      "\n",
      "------------------------------------------------------\n",
      "Testing model MultinomialNB using sub videos\n",
      "  cross-validating\n",
      "    ***\n",
      "    Mean balanced accuracy over 3 folds = 55.9%\n",
      "    done in 0.4 min,\n",
      "\n",
      "------------------------------------------------------\n",
      "Testing model MultinomialNB using sub science\n",
      "  cross-validating\n",
      "    ***\n",
      "    Mean balanced accuracy over 3 folds = 52.1%\n",
      "    done in 0.1 min,\n",
      "\n",
      "------------------------------------------------------\n",
      "Testing model MultinomialNB using sub politics\n",
      "  cross-validating\n",
      "    ***\n",
      "    Mean balanced accuracy over 3 folds = 54.1%\n",
      "    done in 0.3 min,\n",
      "\n",
      "------------------------------------------------------\n",
      "Testing model MultinomialNB using sub politicaldiscussion\n",
      "  cross-validating\n",
      "    ***\n",
      "    Mean balanced accuracy over 3 folds = 72.5%\n",
      "    done in 0.2 min,\n",
      "\n",
      "------------------------------------------------------\n",
      "Testing model MultinomialNB using sub conservative\n",
      "  cross-validating\n",
      "    ***\n",
      "    Mean balanced accuracy over 3 folds = 69.1%\n",
      "    done in 0.1 min,\n",
      "\n",
      "------------------------------------------------------\n",
      "Testing model MultinomialNB using sub the_Donald\n",
      "  cross-validating\n",
      "    ***\n",
      "    Mean balanced accuracy over 3 folds = 70.8%\n",
      "    done in 0.2 min,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# source data folder \n",
    "srcdir = './data_for_models/'\n",
    "\n",
    "# subs to use for this analysis\n",
    "sub2use = ['aww', 'funny', 'todayilearned','askreddit',\n",
    "           'photography', 'gaming', 'videos', 'science',\n",
    "           'politics', 'politicaldiscussion',             \n",
    "           'conservative', 'the_Donald']\n",
    "\n",
    "# apply a threshold to determine toxic vs not toxic\n",
    "thresh = -1\n",
    "\n",
    "# results logfile path\n",
    "logpath = srcdir + 'model_results_log.csv'\n",
    "\n",
    "# name of model\n",
    "modelname = 'MultinomialNB'\n",
    "\n",
    "# specify parameters for text prep\n",
    "processkwargs = {\n",
    "    'stemmer':'snowball', # snowball stemmer\n",
    "    'regexstr':None, # remove all but alphanumeric chars\n",
    "    'lowercase':False, # make lowercase\n",
    "    'removestop':False, # don't remove stop words \n",
    "    'verbose':False\n",
    "                }\n",
    "\n",
    "# Tfidf vectorizer optimized parameters for model\n",
    "tfidfargs = {\n",
    "    \"analyzer\":'word', \n",
    "    \"max_features\" : 10000,\n",
    "    \"max_df\" : 0.53, # Filters out terms that occur in more than half of the docs (max_df=0.5)\n",
    "    \"min_df\" : 2, # Filters out terms that occur in only one document (min_df=2).\n",
    "    \"ngram_range\":(1, 2), # unigrams\n",
    "    \"stop_words\" : \"english\", # None, # \"english\", # Strips out “stop words”\n",
    "    \"use_idf\" : False,\n",
    "    \"sublinear_tf\" : False,\n",
    "    }\n",
    "\n",
    "# validate using all subs \n",
    "for subname in sub2use:\n",
    "    t0 = tstart = time()\n",
    "\n",
    "    print('\\n------------------------------------------------------')\n",
    "    print('Testing model %s using sub %s'%(modelname,subname))\n",
    "    \n",
    "    # load feature data and pre-process comment text\n",
    "    t0 = time()\n",
    "    X_text, X_numeric, y = capstone1_helper.load_feature_data([subname], srcdir, \n",
    "                                             toxic_thresh=thresh, \n",
    "                                             text_prep_args=processkwargs)\n",
    "    # vectorize text\n",
    "    vectorizer = TfidfVectorizer(**tfidfargs)\n",
    "    text_vec = vectorizer.fit_transform(X_text)\n",
    "    \n",
    "    # combine textvec + numeric\n",
    "    dvcols = [s for s in X_numeric.columns if 'dv_' in s ]\n",
    "    cols2use = dvcols + ['u_comment_karma']\n",
    "    # numeric features must be >= 0 \n",
    "    X_numeric[cols2use] = X_numeric[cols2use] - X_numeric[cols2use].min().min()\n",
    "    # concat vactor matrices as sparse array\n",
    "    X = hstack([text_vec.tocsr(), csr_matrix(X_numeric[cols2use])] )\n",
    "    X = X.tocsr()\n",
    "                    \n",
    "    # create clf \n",
    "    clf = MultinomialNB()\n",
    "                \n",
    "    # set model with the optimal hyperparamters\n",
    "    # (I just use defaults for MultinomialNB)\n",
    "#     clf.set_params(**clfparams)\n",
    "                \n",
    "    # do cross validaion\n",
    "    t0 = time()\n",
    "    print('  cross-validating')\n",
    "    cross_validate_classifier(clf, X, y, logpath, modelname, subname, balance=True)\n",
    "    print('    done in %0.1f min,'%((time() - t0)/60))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
