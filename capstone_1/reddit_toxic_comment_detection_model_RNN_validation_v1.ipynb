{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit toxic comment classifier: <br />Recurrent Neural Network\n",
    "## K folds cross-validation over all subs\n",
    "\n",
    "### John Burt\n",
    "\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "The goal of my first Capstone project is to develop a toxic comment classifier. This notebook will train a Recurrent Neural Network classifier to detect toxic Reddit comments, using tuned hyperparameters, and test it with K folds cross-validation. The script will train and test all subreddit datasets in turn and will report performance statistics.\n",
    "\n",
    "### Load the data.\n",
    "\n",
    "The comment data used in this analysis was prepared in three stages:\n",
    "\n",
    "- [acquired using Reddit Python API PRAW](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_collect_comments_v1.ipynb) from 12 subs. 8 of the subs are non-political, and 4 are political in nature. Models are trained on data for only one subreddit at a time, so that they are specialized to that subreddit.\n",
    "\n",
    "\n",
    "- The raw comment metadata was [processed using PCA to produce a single toxicity score](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_generate_PCA_score_v2.ipynb) based on the votes and number of replies. Toxicity score was calculated and normalized within each subreddit and then ranged between -5 and +5 to create a toxicity score comparable between subs. The toxicity score was then thresholded to generate binary \"toxic\" vs. \"not toxic\" labels for supervised model training. The threshold applied was: score <= -1 = \"toxic\", otherwise \"not toxic\". \n",
    "\n",
    "\n",
    "- [Features for training the models were generated](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_comment_create_model_features_v1.ipynb) and saved to two sample aligned feature files for each subreddit. These files are used by the models for input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'capstone1_helper' from 'C:\\\\Users\\\\john\\\\notebooks\\\\reddit\\\\capstone1_helper.py'>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# ---\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "# import helper functions\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "import capstone1_helper\n",
    "import importlib\n",
    "importlib.reload(capstone1_helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text data transformation for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class SequenceText( BaseEstimator, TransformerMixin ):\n",
    "    \"\"\"Custom Transformer that converts text to \n",
    "    tokenized padded sequences\"\"\"\n",
    "    \n",
    "    def __init__( self ):\n",
    "        self._n_most_common_words = 25000 \n",
    "        self._max_seq_len = 100 \n",
    "        self._lowercase = True \n",
    "        self._text_filter = '\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~' \n",
    "        \n",
    "    def set_params(self,**params):\n",
    "        self.__dict__.update(params)\n",
    "    \n",
    "    #Return self nothing else to do here    \n",
    "    def fit( self, X, y = None ):\n",
    "        return self \n",
    "    \n",
    "    #Return self nothing else to do here    \n",
    "    def fit_transform( self, X, y = None ):\n",
    "        return self.transform(X) \n",
    "    \n",
    "    #Method that describes what we need this transformer to do\n",
    "    def transform( self, X, y = None ):\n",
    "        \"\"\"Convert text to tokenized padded sequences\"\"\"\n",
    "        tokenizer = Tokenizer(num_words=self._n_most_common_words, \n",
    "                              filters=self._text_filter, \n",
    "                              lower=self._lowercase)\n",
    "        # input text can be passed in various forms\n",
    "        if type(X) == pd.DataFrame:\n",
    "            text = list(X['text'].values)\n",
    "        elif type(X) == pd.Series:\n",
    "            text = list(X.values)\n",
    "        else:\n",
    "            text = list(X)\n",
    "        tokenizer.fit_on_texts(text)\n",
    "        sequences = tokenizer.texts_to_sequences(text)\n",
    "        # return padded sequences for RNN model training/testing\n",
    "        return pd.DataFrame(pad_sequences(sequences, \n",
    "                                          maxlen=self._max_seq_len))    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "from keras.layers import Dense, Embedding, Dropout, LSTM, SpatialDropout1D, Bidirectional\n",
    "from keras.models import Sequential \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# define RNN model\n",
    "class RNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"RNN classifier model for text classification\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"initialize the classifier \"\"\"\n",
    "        self._n_most_common_words = 25000\n",
    "        self._max_seq_len = 100\n",
    "        self._n_embedding = 16\n",
    "        self._n_lstm = 16\n",
    "        self._dropoutrate = .5\n",
    "        self._clf = None\n",
    "        \n",
    "    def create_model(self):\n",
    "        numoutputs = 1 # this might be a set param someday\n",
    "        # create the classifier model\n",
    "        self._clf = Sequential()\n",
    "        self._clf.add(Embedding(self._n_most_common_words, \n",
    "                                self._n_embedding, input_length=self._max_seq_len))\n",
    "        self._clf.add(SpatialDropout1D(self._dropoutrate))\n",
    "        self._clf.add(Bidirectional(LSTM(self._n_lstm, dropout=self._dropoutrate, \n",
    "                                         recurrent_dropout=self._dropoutrate)))\n",
    "        self._clf.add(Dense(numoutputs, activation='relu'))\n",
    "        self._clf.compile(optimizer='adam', loss='mean_squared_error', \n",
    "                          metrics=[ 'accuracy'])        \n",
    "#         print(self._clf.summary())\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.__dict__.update(params)\n",
    "        self.create_model()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # these are generated outside the hyperopt loop\n",
    "        global X_test, y_test\n",
    "        # local params for this fit run\n",
    "        epochs = 2\n",
    "        batch_size = 2000\n",
    "        # create model if it isn't already made\n",
    "        if self._clf is None:\n",
    "            self.create_model()\n",
    "        # train the classifier with this set of data\n",
    "        return self._clf.fit(X, y, \n",
    "                          epochs=epochs, \n",
    "                          batch_size=batch_size,\n",
    "                          validation_data = (X_test, y_test),\n",
    "                          callbacks=[EarlyStopping(\n",
    "                              monitor='val_loss',patience=1, \n",
    "                              min_delta=0.0001)],verbose=1)\n",
    "#                               min_delta=0.0001), plot_losses],verbose=0)\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return self._clf.predict(X)  \n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        return balanced_accuracy_score(y, self._clf.predict(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# cross-validation of classifier model with text string data X, category labels in y\n",
    "# ** NOTE: X and y must be passed as pandas objects\n",
    "def cross_validate_classifier(clf, X, y, logpath, modelname, subname, balance=True):\n",
    "    \"\"\"Set up kfold to generate several train-test sets, \n",
    "        then train and test\"\"\" \n",
    "    \n",
    "    global X_test, y_test\n",
    "    \n",
    "    if type(X) == pd.DataFrame:\n",
    "        X = X.values\n",
    "              \n",
    "    kf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    i = 1\n",
    "    accuracy = []\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "        # balance label category frequencies by upsampling\n",
    "        if balance:\n",
    "            X_train, y_train = capstone1_helper.balance_classes_np(\n",
    "                X[train_index,:], y[train_index], verbose=False)\n",
    "            \n",
    "        # do not balance category frequencies\n",
    "        else:\n",
    "            X_train = X[train_index,:]\n",
    "            y_train = y[train_index]\n",
    "            \n",
    "        # extract test set for this fold\n",
    "        X_test = X[test_index,:]\n",
    "        y_test = y[test_index]\n",
    "\n",
    "        t0 = time()\n",
    "\n",
    "        # train the model\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # generate predictions for test data\n",
    "        y_est = clf.predict(X_test)\n",
    "        y_pred = (np.where(y_est>.5,1,0))\n",
    "\n",
    "        # log the results\n",
    "        capstone1_helper.log_model_results(logpath, modelname, \n",
    "                                           subname, y_test, y_pred,\n",
    "                                          time()-t0)\n",
    "        \n",
    "        # store the balanced accuracy stat\n",
    "        accuracy.append(balanced_accuracy_score(y_test, y_pred))\n",
    "        i += 1\n",
    "\n",
    "    print(\"\\n    Mean balanced accuracy over %d folds = %2.1f%%\"%(\n",
    "        len(accuracy), np.mean(accuracy)*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test all subs with optimized parameters\n",
    "\n",
    "This script will validation test a model for all subreddit datasets, using hyperparameters optimized with hyperopt in a previous notebook. The model will be K folds cross-validated with data for each subreddit, and the results will be saved to a common logfile so that cross-model comparisons can be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------\n",
      "Testing model RNN using sub aww\n",
      "  cross-validating\n",
      "Train on 276402 samples, validate on 72454 samples\n",
      "Epoch 1/2\n",
      "276402/276402 [==============================] - 298s 1ms/step - loss: 0.2713 - acc: 0.5319 - val_loss: 0.2334 - val_acc: 0.6231\n",
      "Epoch 2/2\n",
      "276402/276402 [==============================] - 302s 1ms/step - loss: 0.2165 - acc: 0.6588 - val_loss: 0.2063 - val_acc: 0.6928\n",
      "Train on 276402 samples, validate on 72453 samples\n",
      "Epoch 1/2\n",
      "276402/276402 [==============================] - 310s 1ms/step - loss: 0.2141 - acc: 0.6676 - val_loss: 0.1990 - val_acc: 0.7085\n",
      "Epoch 2/2\n",
      "276402/276402 [==============================] - 310s 1ms/step - loss: 0.1915 - acc: 0.7165 - val_loss: 0.1899 - val_acc: 0.7228\n",
      "Train on 276404 samples, validate on 72452 samples\n",
      "Epoch 1/2\n",
      "276404/276404 [==============================] - 307s 1ms/step - loss: 0.1963 - acc: 0.7074 - val_loss: 0.1884 - val_acc: 0.7164\n",
      "Epoch 2/2\n",
      "276404/276404 [==============================] - 308s 1ms/step - loss: 0.1779 - acc: 0.7431 - val_loss: 0.1825 - val_acc: 0.7259\n",
      "\n",
      "    Mean balanced accuracy over 3 folds = 64.0%\n",
      "    done in 33.8 min,\n",
      "\n",
      "------------------------------------------------------\n",
      "Testing model RNN using sub funny\n",
      "  cross-validating\n",
      "Train on 329052 samples, validate on 85142 samples\n",
      "Epoch 1/2\n",
      "324000/329052 [============================>.] - ETA: 5s - loss: 0.2594 - acc: 0.5656"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-be2fb20b0015>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'  cross-validating'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     cross_validate_classifier(clf, X, y, logpath, \n\u001b[1;32m---> 77\u001b[1;33m                               modelname, subname, balance=True)\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'    done in %0.1f min,'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-3e88144dd156>\u001b[0m in \u001b[0;36mcross_validate_classifier\u001b[1;34m(clf, X, y, logpath, modelname, subname, balance)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# generate predictions for test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-f9506bbea7c0>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     54\u001b[0m                           callbacks=[EarlyStopping(\n\u001b[0;32m     55\u001b[0m                               \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                               min_delta=0.0001)],verbose=1)\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;31m#                               min_delta=0.0001), plot_losses],verbose=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datasci\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datasci\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datasci\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datasci\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datasci\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.sparse import vstack, hstack\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "# source data folder \n",
    "srcdir = './data_for_models/'\n",
    "\n",
    "# subs to use for this analysis\n",
    "sub2use = ['aww', 'funny', 'todayilearned','askreddit',\n",
    "           'photography', 'gaming', 'videos', 'science',\n",
    "           'politics', 'politicaldiscussion',             \n",
    "           'conservative', 'the_Donald']\n",
    "\n",
    "# apply a threshold to determine toxic vs not toxic\n",
    "thresh = -1\n",
    "\n",
    "# results logfile path\n",
    "logpath = srcdir + 'model_results_log.csv'\n",
    "\n",
    "# name of model\n",
    "modelname = 'RNN'\n",
    "\n",
    "# specify parameters for text prep\n",
    "processkwargs = {\n",
    "    'stemmer':'snowball', # snowball stemmer\n",
    "    'regexstr':None, # remove all but alphanumeric chars\n",
    "    'lowercase':False, # make lowercase\n",
    "    'removestop':False, # don't remove stop words \n",
    "    'verbose':False\n",
    "                }\n",
    "# text to sequence transformer for model input\n",
    "text2seqargs = {\n",
    "    \"n_most_common_words\" : 20000, \n",
    "    \"max_seq_len\" : 4, \n",
    "    \"lowercase\" : True, \n",
    "    \"text_filter\" : '\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', \n",
    "    }\n",
    "\n",
    "# model optimized parameters\n",
    "clfargs = {\n",
    "    # these two must be same as text2seqargs!\n",
    "    \"n_most_common_words\": text2seqargs['n_most_common_words'], \n",
    "    \"max_seq_len\" : text2seqargs['max_seq_len'],  \n",
    "    \"n_embedding\" : 7, \n",
    "    \"n_lstm\" : 10, \n",
    "    \"dropoutrate\" : 0.44, \n",
    "    }\n",
    "\n",
    "# validate using all subs \n",
    "for subname in sub2use:\n",
    "    t0 = tstart = time()\n",
    "\n",
    "    print('\\n------------------------------------------------------')\n",
    "    print('Testing model %s using sub %s'%(modelname,subname))\n",
    "    \n",
    "    # load feature data and pre-process comment text\n",
    "    t0 = time()\n",
    "    X_text, X_numeric, y = capstone1_helper.load_feature_data([subname], srcdir, \n",
    "                                             toxic_thresh=thresh, \n",
    "                                             text_prep_args=processkwargs)\n",
    "    # sequence convert text\n",
    "    text2seq = SequenceText()\n",
    "    text2seq.set_params(**text2seqargs)\n",
    "    X = text2seq.fit_transform(X_text)\n",
    "                        \n",
    "    # create clf \n",
    "    clf = RNNClassifier()\n",
    "                \n",
    "    # set model with the optimal hyperparamters\n",
    "    clf.set_params(**clfargs, n_jobs=4)\n",
    "                \n",
    "    # do cross validaion\n",
    "    t0 = time()\n",
    "    print('  cross-validating')\n",
    "    cross_validate_classifier(clf, X, y, logpath, \n",
    "                              modelname, subname, balance=True)\n",
    "    print('    done in %0.1f min,'%((time() - t0)/60))\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
