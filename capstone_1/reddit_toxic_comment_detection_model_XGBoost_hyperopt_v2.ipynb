{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit toxic comment classifier: <br />XGBoost\n",
    "## Hyperparameter tuning with hyperopt\n",
    "\n",
    "### John Burt\n",
    "\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "The goal of my first Capstone project is to develop a toxic comment classifier. This notebook will implement an Extreme Gradient Boosting classifier (XGBoost) and tune hyperparameters using the hyperopt Baysian hyperparameter optimization package.\n",
    "\n",
    "**Note:** for several other model hypertuning scripts, I'm using a pipeline as the estimator for tuning, which allows me to tune TfidfVectorizer params in addition to the classifier. However, with XGBoostClassifier that adds too much time to an already long tuning run, so here I only tune XGBoostClassifier params (the process still takes most of a day!).\n",
    "\n",
    "### The data.\n",
    "\n",
    "The comment data used in this analysis was prepared in three stages:\n",
    "\n",
    "- [acquired using Reddit Python API PRAW](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_collect_comments_v1.ipynb) from 12 subs. 8 of the subs are non-political, and 4 are political in nature. Models are trained on data for only one subreddit at a time, so that they are specialized to that subreddit.\n",
    "\n",
    "\n",
    "- The raw comment metadata was [processed using PCA to produce a single toxicity score](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_generate_PCA_score_v2.ipynb) based on the votes and number of replies. Toxicity score was calculated and normalized within each subreddit and then ranged between -5 and +5 to create a toxicity score comparable between subs. The toxicity score was then thresholded to generate binary \"toxic\" vs. \"not toxic\" labels for supervised model training. The threshold applied was: score <= -1 = \"toxic\", otherwise \"not toxic\". \n",
    "\n",
    "\n",
    "- [Features for training the models were generated](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_comment_create_model_features_v1.ipynb) and saved to two sample aligned feature files for each subreddit. These files are used by the models for input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1dc9e9fd2b6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import glob\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from time import time\n",
    "from hyperopt import space_eval\n",
    "from hyperopt import tpe, hp, fmin, Trials\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4cadcfb2857b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# ---\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2312\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2313\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2314\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2315\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<C:\\Users\\john\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\decorator.py:decorator-gen-109>\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\IPython\\core\\magics\\pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m             \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[1;34m(self, gui)\u001b[0m\n\u001b[0;32m   3400\u001b[0m         \"\"\"\n\u001b[0;32m   3401\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpylabtools\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3402\u001b[1;33m         \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3404\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36mfind_gui_and_backend\u001b[1;34m(gui, gui_select)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \"\"\"\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgui\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# ---\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "# import helper functions\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "import capstone1_helper\n",
    "import importlib\n",
    "importlib.reload(capstone1_helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization using Baysian methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from time import time\n",
    "from hyperopt import space_eval\n",
    "from hyperopt import tpe, hp, fmin, Trials\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# source data folder \n",
    "srcdir = './data_for_models/'\n",
    "\n",
    "# subs to use for this analysis\n",
    "# sub2use = ['gaming', 'politics']\n",
    "sub2use = ['politics','gaming']\n",
    "\n",
    "# apply a threshold to determine toxic vs not toxic\n",
    "thresh = -1\n",
    "\n",
    "# results logfile path\n",
    "logpath = srcdir + 'model_hyperopt_results_log.csv'\n",
    "\n",
    "# name of model\n",
    "modelname = 'XGBoost_3'\n",
    "    \n",
    "# specify parameters for text prep\n",
    "processkwargs = {\n",
    "    'stemmer':'snowball', # snowball stemmer\n",
    "    'regexstr':None, # remove all but alphanumeric chars\n",
    "    'lowercase':False, # make lowercase\n",
    "    'removestop':False, # don't remove stop words \n",
    "    'verbose':False\n",
    "                }\n",
    "\n",
    "# define model defaults for TF-IDF vectorizer.\n",
    "tfidfargs = {\n",
    "    \"analyzer\":'word', \n",
    "    \"max_features\" : 10000,\n",
    "    \"max_df\" : 0.5, # Filters out terms that occur in more than half of the docs (max_df=0.5)\n",
    "    \"min_df\" : 5, # Filters out terms that occur in only one document (min_df=2).\n",
    "    \"ngram_range\":(1, 3), # unigrams\n",
    "    \"stop_words\" : 'english',   #None, #\"english\", # Strips out “stop words”\n",
    "    \"use_idf\" : True\n",
    "    }\n",
    "\n",
    "# hyperopt objective function\n",
    "def objective(params):\n",
    "    global X_train_bal, y_train_bal\n",
    "    clf = XGBClassifier(tree_method='gpu_hist')\n",
    "#     print('objective func params:',params)\n",
    "#     clf.set_params(**params)\n",
    "#     print('\\nclf.get_params()', clf.get_params()['steps'][1])\n",
    "    score = cross_val_score(clf, X_train_bal, y_train_bal, \n",
    "                            scoring='balanced_accuracy',n_jobs=2).mean()   \n",
    "    return 1-score\n",
    "   \n",
    "# hyperopt parameter space\n",
    "paramspace = {\n",
    "#     'pre__tfidf__stop_words': hp.choice('tfidf__stop_words', ['english', None]),\n",
    "#     'pre__tfidf__use_idf': hp.choice('tfidf__use_idf', [True, False]),\n",
    "#     'pre__tfidf__sublinear_tf': hp.choice('tfidf__sublinear_tf', [True, False]),\n",
    "#     'pre__tfidf__min_df': 1+hp.randint('tfidf__min_df', 5),\n",
    "#     'pre__tfidf__max_df': hp.uniform('tfidf__max_df', 0.5, 1.0),\n",
    "#     'pre__tfidf__ngram_range': hp.choice('tfidf__ngram_range', [(1, 1),(1, 2), (1, 3)]),\n",
    "    \n",
    "    'clf__learning_rate': hp.uniform('clf__learning_rate', 0.01, 1.0),\n",
    "    'clf__max_depth': 3+hp.randint('clf__max_depth', 7),\n",
    "    'clf__n_estimators': 100+hp.randint('clf__n_estimators', 1000),\n",
    "    'clf__min_child_weight': 1+hp.randint('clf__min_child_weight', 10),\n",
    "    'clf__scale_pos_weight': 1+hp.randint('clf__scale_pos_weight', 5),\n",
    "    }\n",
    "\n",
    "# loop through to tune model with comments from each specified subreddit \n",
    "for subname in sub2use:\n",
    "    t0 = tstart = time()\n",
    "\n",
    "    print('------------------------------------------------------')\n",
    "    print('\\nTuning model %s using sub %s'%(modelname,subname))\n",
    "    \n",
    "    print('  loading feature data')\n",
    "    t0 = time()\n",
    "    X_text, X_numeric, y = capstone1_helper.load_feature_data([subname], srcdir, \n",
    "                                             toxic_thresh=thresh, \n",
    "                                             text_prep_args=processkwargs)\n",
    "    print('    done in %0.3fs,'%(time() - t0),'X_text, X_numeric, y:',X_text.shape, X_numeric.shape, y.shape)\n",
    "    \n",
    "    # vectorize the text\n",
    "    print('  vectorizing text')\n",
    "    vectorizer = TfidfVectorizer(**tfidfargs)\n",
    "    text_vec = vectorizer.fit_transform(X_text)\n",
    "\n",
    "    # combine textvec + numeric\n",
    "    dvcols = [s for s in X_numeric.columns if 'dv_' in s ]\n",
    "    cols2use = dvcols + ['u_comment_karma']\n",
    "    # numeric features must be >= 0 \n",
    "    X_numeric[cols2use] = X_numeric[cols2use] - X_numeric[cols2use].min().min()\n",
    "    # concat vactor matrices as sparse array\n",
    "    X = hstack([text_vec.tocsr(), csr_matrix(X_numeric[cols2use])] )\n",
    "    X = X.tocsr()\n",
    "    print('text_vec, X_numeric[cols2use], X',\n",
    "          text_vec.shape, X_numeric[cols2use].shape, X.shape)\n",
    "    \n",
    "    # Split into test and training data\n",
    "    t0 = time()\n",
    "    print('  train/test split')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.1, random_state=42)\n",
    "    print('    done in %0.3fs,'%(time() - t0),'X_train, X_test', X_train.shape, X_test.shape)\n",
    "    \n",
    "    # upsample training data to balance classes\n",
    "    print('  balancing training data')\n",
    "    X_train_bal, y_train_bal = capstone1_helper.balance_classes_sparse(\n",
    "        X_train, y_train, verbose=False)\n",
    "   \n",
    "    # hyperparameter tuning:\n",
    "    # The Trials object will store details of each iteration\n",
    "    trials = Trials()\n",
    "\n",
    "    # Run the hyperparameter search using the tpe algorithm\n",
    "    t0 = time()\n",
    "    print('  tune model')\n",
    "    best = fmin(fn=objective, space=paramspace, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "    print('    done in %0.3fs,'%(time() - t0))\n",
    "\n",
    "    # Get the values of the optimal parameters\n",
    "    best_params = space_eval(paramspace, best)\n",
    "    print('\\n  Best parameters:',best_params)   \n",
    "\n",
    "    # test model\n",
    "    clf = XGBClassifier()\n",
    "\n",
    "    # set model with the optimal hyperparamters\n",
    "    clf.set_params(**best_params)\n",
    "\n",
    "    # fit the classifier with training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # y_out = clf.predict_proba(X_test)[:,1]\n",
    "    y_out = clf.predict(X_test)\n",
    "    y_pred = (np.where(y_out>.5,1,0))\n",
    "    \n",
    "    # log the results\n",
    "    capstone1_helper.log_model_results(logpath, modelname, subname, y_test, y_pred)\n",
    "\n",
    "    print('\\n  model performance:')\n",
    "    print('    Test set: #nontoxic =', (y_test==0).sum(), ' #toxic =', (y_test==1).sum())\n",
    "    print('    overall accuracy: %1.3f'%(np.sum((y_pred==y_test))/y_test.shape[0]))\n",
    "    print('    Precision: %1.3f'%(precision_score(y_test, y_pred)))\n",
    "    print('    Recall: %1.3f'%(recall_score(y_test, y_pred)))\n",
    "    print('    Balanced Accuracy: %1.3f'%(balanced_accuracy_score(y_test, y_pred)))\n",
    "    print('    F1 Score: %1.3f'%(f1_score(y_test, y_pred)))\n",
    "    print('    ROC AUC: %1.3f'%(roc_auc_score(y_test, y_pred)))    \n",
    "    \n",
    "    print('\\n  Total time to optimize model for sub %s = %3.1f min'%(subname, (time() - tstart)/60))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
