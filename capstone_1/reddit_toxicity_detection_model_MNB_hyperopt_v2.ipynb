{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit comment toxicity classifier: Multinomial Naive Bayes\n",
    "# with hyperopt param tuning\n",
    "\n",
    "### John Burt\n",
    "\n",
    "[To hide code cells, view this in nbviewer](https://nbviewer.jupyter.org/github/johnmburt/springboard/blob/master/capstone_1/reddit_toxicity_detection_model_MNB_v1.ipynb) \n",
    "\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "The goal of my first Capstone project is to develop a toxic comment classifier. This notebook will implement and HP tune a Multinomial Naive Bayes classifier.\n",
    "\n",
    "\n",
    "to do:\n",
    "- X autosave prepped text versions\n",
    "- X pull out holdout set before training\n",
    "- X balance classes\n",
    "- generate features:\n",
    "  - user karma\n",
    "  - mean user score by sub\n",
    "- hyperopt\n",
    "- confusion matrix with model results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data.\n",
    "\n",
    "The comment data used in this analysis was [acquired using Reddit Python API PRAW](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_collect_comments_v1.ipynb) from 12 subs. 8 of the subs are non-political, and 4 are political in nature. \n",
    "\n",
    "The raw comment data was [processed using PCA to produce a single toxicity score](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_generate_PCA_score_v1.ipynb) based on the votes and number of replies. \n",
    "\n",
    "Then I [converted this score into an integer 0 to 4 range training label variable](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_create_train-test_set.ipynb), with 0 being no/low toxicity and higher values indicating higher toxicity. \n",
    "\n",
    "Note that this is a highly unbalanced dataset, with less than 10% of comments having toxicity label values above 0. I'll have to adjust this proportion for models that require reasonably balanced categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# ---\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the prepared comment data file,\n",
    "## or prepare original comment text and save\n",
    "\n",
    "\n",
    "- Try to load a pre-saved data file that contains processed comment text. \n",
    "- If the pre-saved comment file doesn't exist then: \n",
    "  - Clean up text and prepare it for NLP model training. \n",
    "  - Save the processed text so that it can be read in later sessions without waiting.\n",
    "  - In the next session, the pre-processed data file will be read if it exists, otherwise the text will be pre-processed and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "# function to prepare text for NLP analysis\n",
    "def process_comment_text(comments, \n",
    "                         stemmer=None, \n",
    "                         regexstr=None, lowercase=True,\n",
    "                         removestop=False,\n",
    "                         verbose=True):\n",
    "    \"\"\"Helper function to pre-process text.\n",
    "        Combines several preprocessing steps: lowercase, \n",
    "            remove stop, regex text cleaning, stemming\"\"\"\n",
    "    \n",
    "    if type(stemmer) == str:\n",
    "        if stemmer.lower() == 'porter':\n",
    "            stemmer = PorterStemmer()\n",
    "        elif stemmer.lower() == 'snowball':\n",
    "            stemmer = SnowballStemmer(language='english')\n",
    "        else:\n",
    "            stemmer = None\n",
    "            \n",
    "    processed = comments\n",
    "    \n",
    "    # make text lowercase\n",
    "    if lowercase == True:\n",
    "        if verbose: print('make text lowercase')\n",
    "        processed = processed.str.lower()\n",
    "        \n",
    "    # remove stop words\n",
    "    # NOTE: stop words w/ capitals not removed!\n",
    "    if removestop == True:\n",
    "        if verbose: print('remove stop words')\n",
    "        stopwords = sw.words(\"english\")\n",
    "        processed = processed.map(lambda text: ' '.join([word for word in text.split() if word not in stopwords]))\n",
    "        \n",
    "    # apply regex expression\n",
    "    if regexstr is not None:\n",
    "        if verbose: print('apply regex expression')\n",
    "        regex = re.compile(regexstr) \n",
    "        processed = processed.str.replace(regex,' ')\n",
    "        \n",
    "    # stemming\n",
    "    # NOTE: stemming makes all lowercase\n",
    "    if stemmer is not None:\n",
    "        if verbose: print('stemming')\n",
    "        processed = processed.map(lambda x: ' '.join([stemmer.stem(y) for y in x.split(' ')]))\n",
    "        \n",
    "    if verbose: print('done')\n",
    "         \n",
    "    return processed\n",
    "\n",
    "\n",
    "from os import path\n",
    "\n",
    "# source data folder \n",
    "srcdir = './data_labeled/'\n",
    "\n",
    "# csv filename\n",
    "inputname = 'comment_sample_train-test_data'\n",
    "\n",
    "# specify parameters for text prep\n",
    "# NOTE: if these are changed, then a new prepped text file \n",
    "#  will be created.\n",
    "processkwargs = {\n",
    "    'stemmer':'snowball', # snowball stemmer\n",
    "    'regexstr':'[^a-zA-Z0-9\\s]', # remove all but alphanumeric chars\n",
    "    'lowercase':True, # make lowercase\n",
    "    'removestop':False # don't remove stop words \n",
    "                }\n",
    "\n",
    "# create name for prepared text file\n",
    "prepfile = inputname+' stem=%s, regex=%s, lower=%d, stop=%d.csv'%(\n",
    "    processkwargs['stemmer'], \n",
    "    processkwargs['regexstr'].replace('\\\\','(sl)'), \n",
    "    processkwargs['lowercase'], \n",
    "    processkwargs['removestop'])\n",
    "\n",
    "preppath = srcdir+prepfile\n",
    "\n",
    "# if prepared text file exists, just load it\n",
    "# if False:\n",
    "if path.exists(preppath):\n",
    "    print('reading prepped text file: ', prepfile)\n",
    "    df = pd.read_csv(preppath).drop_duplicates()\n",
    "    \n",
    "# if prepared file doesn't exist, then create it and save it\n",
    "else:\n",
    "    print('no prepped file, so reading original and prepping text...')\n",
    "    # read the original\n",
    "    origpath = srcdir+inputname+'.csv'\n",
    "    df = pd.read_csv(origpath).drop_duplicates()\n",
    "    # process text, make that the text version of the training data\n",
    "    verbose = True\n",
    "    df['text_prep'] = process_comment_text(df['text'], **processkwargs, verbose=verbose)\n",
    "    # save prepared text file\n",
    "    df.to_csv(preppath,index=False)\n",
    "    print('saved prepped text data to',preppath)\n",
    "    \n",
    "# remove any rows with nans\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# remove any rows with no text after pre-processing\n",
    "df = df[df['text_prep']!='']\n",
    "\n",
    "# select comments from the specified subs\n",
    "subs2use = ['gaming']\n",
    "df = df[df['sub_name'].str.contains('|'.join(subs2use))]\n",
    "print(df.shape)\n",
    "\n",
    "print('\\nTotal comment samples read:',df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading prepped text file:  comment_sample_train-test_data stem=snowball, regex=[^a-zA-Z0-9(sl)s], lower=1, stop=0.csv\n",
      "(389944, 24)\n",
      "\n",
      "Total comment samples read: 389944\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the feature and label data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350949 training samples, 38995 test samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# the label used = 0-4 scale, w/ 4 = most toxic\n",
    "# y = df['label_neg-inv']\n",
    "\n",
    "# use the binary label: 0 = not toxic, 1 = toxic\n",
    "y = df['label_bin']\n",
    "\n",
    "# clip ranges\n",
    "y[y<0] = 0\n",
    "y[y>1] = 1\n",
    "\n",
    "# prepared comment text to use for model\n",
    "X = df['text_prep']\n",
    "\n",
    "# Split into test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print('%d training samples, %d test samples'%(X_train.shape[0],X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance sample frequencies in training samples\n",
    "\n",
    "The classifier may require balancing of sample frequencies between classes for best results.\n",
    "\n",
    "This function will up-sample to the specified number of samples per class.\n",
    "\n",
    "The balance_classes_sparse function does sample balancing with sparse matrices, such as vectorized BOW data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(df, ycol, samples_per_class=None, verbose=False):\n",
    "    \"\"\"Equalize number of samples so that all classes have equal numbers of samples.\n",
    "    If samples_per_class==None, then upsample (randomly repeat) all classes to the largest class,\n",
    "      Otherwise, set samples for all classes to samples_per_class.\"\"\"\n",
    "    \n",
    "    if verbose: print('Balancing class sample frequencies:')\n",
    "    \n",
    "    # all class IDs\n",
    "    classes =  df[ycol].unique()\n",
    "    classes = classes[~np.isnan(classes)]\n",
    "    \n",
    "    # get class with max samples\n",
    "    if verbose: print('\\tOriginal sample frequencies:')\n",
    "    if samples_per_class is None:\n",
    "        samples_per_class = 0\n",
    "        for c in classes:\n",
    "            if verbose: print('\\t\\tclass:',c,'#samples:',(df[ycol]==c).sum())\n",
    "            samples_per_class = np.max([samples_per_class, (df[ycol]==c).sum()])\n",
    "    if verbose: print('\\tNew samples_per_class:',samples_per_class)\n",
    "            \n",
    "    # create a list of samples for each class with equal sample numbers \n",
    "    newdata = []\n",
    "    for c in classes:\n",
    "        newdata.append(df[df[ycol]==c].sample(samples_per_class, replace=True)) \n",
    "\n",
    "    return pd.concat(newdata)\n",
    "\n",
    "# ******************************************************************************************\n",
    "from scipy.sparse import vstack, hstack\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "def balance_classes_sparse(X, y, samples_per_class=None, verbose=False):\n",
    "    \"\"\"Equalize number of samples so that all classes have equal numbers of samples.\n",
    "    If samples_per_class==None, then upsample (randomly repeat) all classes to the largest class,\n",
    "      Otherwise, set samples for all classes to samples_per_class.\"\"\"\n",
    "    \n",
    "    def get_samples(arr, numsamples):\n",
    "        if arr.shape[0] >= numsamples:\n",
    "            index = np.arange(arr.shape[0])\n",
    "            np.random.shuffle(index)\n",
    "            return arr[index[:numsamples],:]\n",
    "        else:\n",
    "            samples = arr.copy()\n",
    "            numrepeats = int(numsamples / arr.shape[0])\n",
    "            lastsize = numsamples % arr.shape[0]\n",
    "            for i in range(numrepeats-1):\n",
    "                samples = vstack([samples,arr])\n",
    "            if lastsize > 0:\n",
    "                index = np.arange(arr.shape[0])\n",
    "                np.random.shuffle(index)\n",
    "                samples = vstack([samples, arr[index[:lastsize],:]])\n",
    "            return samples   \n",
    "    \n",
    "    if verbose: \n",
    "        print('Balancing class sample frequencies:')\n",
    "        \n",
    "    # all class IDs\n",
    "    classes =  pd.unique(y)\n",
    "    classes = classes[~np.isnan(classes)]\n",
    "    \n",
    "    # get class with max samples\n",
    "    if verbose: \n",
    "        print('\\tOriginal sample frequencies:')\n",
    "    if samples_per_class is None:\n",
    "        samples_per_class = 0\n",
    "        for c in classes:\n",
    "            if verbose: \n",
    "                print('\\t\\tclass:',c,'#samples:',(np.sum(y==c)))\n",
    "            samples_per_class = np.max([samples_per_class, np.sum(y==c)])\n",
    "    if verbose: \n",
    "        print('\\tNew samples_per_class:',samples_per_class)\n",
    "                              \n",
    "    # combine X and y\n",
    "    Xy = csr_matrix(hstack([X, np.reshape(y, (-1, 1))]))\n",
    "       \n",
    "    # create a list of samples for each class with equal sample numbers \n",
    "    newdata = None\n",
    "    for c in classes:\n",
    "        if newdata is None:\n",
    "            newdata = get_samples(Xy[y==c,:], samples_per_class)\n",
    "        else:\n",
    "            newdata = vstack([newdata, get_samples(Xy[y==c,:], samples_per_class)])\n",
    "            \n",
    "    return newdata[:,:-1], newdata[:,-1].toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization using Baysian methods\n",
    "\n",
    "\n",
    "Note: this article provided useful code for pipelines with hyperopt:\n",
    "\n",
    "- [Hyperparameter Tuning with hyperopt in Python](http://steventhornton.ca/blog/hyperparameter-tuning-with-hyperopt-in-python.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 100/100 [45:50<00:00, 34.29s/it, best loss: -0.6331680292740242]\n",
      "best:\n",
      "{'tfidf__max_df': 0.8634865274084329, 'tfidf__min_df': 0, 'tfidf__ngram_range': 0, 'tfidf__stop_words': 1, 'tfidf__sublinear_tf': 1, 'tfidf__use_idf': 1}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import tpe, hp, fmin, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from time import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# create a custom classifier that balances the training data\n",
    "class BalancedClf(MultinomialNB):\n",
    "    \"\"\"Wrapper class that balances data by upsampling prior to training\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        bal_X, bal_y = balance_classes_sparse(X, y, verbose=False)\n",
    "        super().fit(bal_X, bal_y, **fit_params)\n",
    "        return self\n",
    "    \n",
    "# objective function \n",
    "def objective(params):\n",
    "#     print('params:',params)  \n",
    "    clf = Pipeline([    \n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('clf', BalancedClf())])    \n",
    "    clf.set_params(**params)    \n",
    "    score = cross_val_score(clf, X_train.values, y_train.values, \n",
    "                            scoring='balanced_accuracy',n_jobs=4).mean()   \n",
    "    return -score\n",
    "\n",
    "# parameter space\n",
    "paramspace = {\n",
    "    'tfidf__stop_words': hp.choice('tfidf__stop_words', ['english', None]),\n",
    "    'tfidf__use_idf': hp.choice('tfidf__use_idf', [True, False]),\n",
    "    'tfidf__sublinear_tf': hp.choice('tfidf__sublinear_tf', [True, False]),\n",
    "    'tfidf__min_df': 1+hp.randint('tfidf__min_df', 5),\n",
    "    'tfidf__max_df': hp.uniform('tfidf__max_df', 0.5, 1.0),\n",
    "    'tfidf__ngram_range': hp.choice('tfidf__ngram_range', [(1, 1), (1, 3)])\n",
    "    }\n",
    "\n",
    "# The Trials object will store details of each iteration\n",
    "trials = Trials()\n",
    "\n",
    "# Run the hyperparameter search using the tpe algorithm\n",
    "best = fmin(fn=objective, space=paramspace, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "print('best:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
