{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit comment toxicity classifier: Logistic Regression\n",
    "\n",
    "### John Burt\n",
    "\n",
    "[To hide code cells, view this in nbviewer](https://nbviewer.jupyter.org/github/johnmburt/springboard/blob/master/capstone_1/reddit_toxicity_detection_model_logregress_v1.ipynb) \n",
    "\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "The goal of my first Capstone project is to develop a toxic comment classifier. Logistic Regression is one of the simpler models and will serve as a baseline to compare with more complicated models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data.\n",
    "\n",
    "The comment data used in this analysis was [acquired using Reddit Python API PRAW](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_collect_comments_v1.ipynb) from 12 subs. 8 of the subs are non-political, and 4 are political in nature. \n",
    "\n",
    "The raw comment data was [processed using PCA to produce a single toxicity score](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_generate_PCA_score_v1.ipynb) based on the votes and number of replies. \n",
    "\n",
    "Then I [converted this score into an integer 0 to 4 range training label variable](https://github.com/johnmburt/springboard/blob/master/capstone_1/reddit_create_train-test_set.ipynb), with 0 being no/low toxicity and higher values indicating higher toxicity. \n",
    "\n",
    "Note that this is a highly unbalanced dataset, with less than 10% of comments having toxicity label values above 0. I'll have to adjust this proportion for models that require reasonably balanced categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total comment samples read: 3251323\n"
     ]
    }
   ],
   "source": [
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# ---\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "\n",
    "# source data folder \n",
    "srcdir = './data_labeled/'\n",
    "\n",
    "df = pd.read_csv(srcdir+'comment_sample_train-test_data.csv').drop_duplicates()\n",
    "\n",
    "print('\\nTotal comment samples read:',df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment_ID</th>\n",
       "      <th>sub_name</th>\n",
       "      <th>post_ID</th>\n",
       "      <th>parent_ID</th>\n",
       "      <th>time</th>\n",
       "      <th>age_re_post</th>\n",
       "      <th>age_re_now</th>\n",
       "      <th>u_id</th>\n",
       "      <th>u_name</th>\n",
       "      <th>u_created</th>\n",
       "      <th>u_comment_karma</th>\n",
       "      <th>u_link_karma</th>\n",
       "      <th>num_replies</th>\n",
       "      <th>controversy</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>score_sign</th>\n",
       "      <th>u_days</th>\n",
       "      <th>pca_score</th>\n",
       "      <th>label_neg-pos</th>\n",
       "      <th>label_neg-inv</th>\n",
       "      <th>label_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>e2pe37x</td>\n",
       "      <td>aww</td>\n",
       "      <td>90bu6w</td>\n",
       "      <td>90bu6w</td>\n",
       "      <td>1.532057e+09</td>\n",
       "      <td>5636.0</td>\n",
       "      <td>2.016460e+07</td>\n",
       "      <td>ktsxr</td>\n",
       "      <td>hppmoep</td>\n",
       "      <td>1.421736e+09</td>\n",
       "      <td>64801.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3864.0</td>\n",
       "      <td>He judged the hell out of you and decided you ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1276.865880</td>\n",
       "      <td>4.467621</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>e2p8yc3</td>\n",
       "      <td>aww</td>\n",
       "      <td>90bu6w</td>\n",
       "      <td>90bu6w</td>\n",
       "      <td>1.532051e+09</td>\n",
       "      <td>81.0</td>\n",
       "      <td>2.017015e+07</td>\n",
       "      <td>1f5xz4a2</td>\n",
       "      <td>wcollins260</td>\n",
       "      <td>1.527828e+09</td>\n",
       "      <td>139463.0</td>\n",
       "      <td>157372.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10039.0</td>\n",
       "      <td>You may have saved his little life.</td>\n",
       "      <td>positive</td>\n",
       "      <td>48.880683</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>e2pbfft</td>\n",
       "      <td>aww</td>\n",
       "      <td>90bu6w</td>\n",
       "      <td>90bu6w</td>\n",
       "      <td>1.532054e+09</td>\n",
       "      <td>2716.0</td>\n",
       "      <td>2.016752e+07</td>\n",
       "      <td>d7o70</td>\n",
       "      <td>firmkillernate</td>\n",
       "      <td>1.379584e+09</td>\n",
       "      <td>64836.0</td>\n",
       "      <td>12482.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21666.0</td>\n",
       "      <td>*Moisturize me*</td>\n",
       "      <td>positive</td>\n",
       "      <td>1764.699803</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>e2p9dox</td>\n",
       "      <td>aww</td>\n",
       "      <td>90bu6w</td>\n",
       "      <td>90bu6w</td>\n",
       "      <td>1.532052e+09</td>\n",
       "      <td>527.0</td>\n",
       "      <td>2.016971e+07</td>\n",
       "      <td>bx40q</td>\n",
       "      <td>wyslan</td>\n",
       "      <td>1.370334e+09</td>\n",
       "      <td>11596.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43126.0</td>\n",
       "      <td>Frogs drink through their skin, so you cooled ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1871.736076</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>e2pb7zl</td>\n",
       "      <td>aww</td>\n",
       "      <td>90bu6w</td>\n",
       "      <td>90bu6w</td>\n",
       "      <td>1.532054e+09</td>\n",
       "      <td>2498.0</td>\n",
       "      <td>2.016773e+07</td>\n",
       "      <td>167k31</td>\n",
       "      <td>VioletVenable</td>\n",
       "      <td>1.489587e+09</td>\n",
       "      <td>73050.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15163.0</td>\n",
       "      <td>Its the happy wriggle that does me in.</td>\n",
       "      <td>positive</td>\n",
       "      <td>491.508611</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 comment_ID sub_name post_ID parent_ID          time  \\\n",
       "0         0.0    e2pe37x      aww  90bu6w    90bu6w  1.532057e+09   \n",
       "1         1.0    e2p8yc3      aww  90bu6w    90bu6w  1.532051e+09   \n",
       "2         2.0    e2pbfft      aww  90bu6w    90bu6w  1.532054e+09   \n",
       "3         3.0    e2p9dox      aww  90bu6w    90bu6w  1.532052e+09   \n",
       "4         4.0    e2pb7zl      aww  90bu6w    90bu6w  1.532054e+09   \n",
       "\n",
       "   age_re_post    age_re_now      u_id          u_name     u_created  \\\n",
       "0       5636.0  2.016460e+07     ktsxr         hppmoep  1.421736e+09   \n",
       "1         81.0  2.017015e+07  1f5xz4a2     wcollins260  1.527828e+09   \n",
       "2       2716.0  2.016752e+07     d7o70  firmkillernate  1.379584e+09   \n",
       "3        527.0  2.016971e+07     bx40q          wyslan  1.370334e+09   \n",
       "4       2498.0  2.016773e+07    167k31   VioletVenable  1.489587e+09   \n",
       "\n",
       "   u_comment_karma  u_link_karma  num_replies  controversy    score  \\\n",
       "0          64801.0         444.0         31.0          0.0   3864.0   \n",
       "1         139463.0      157372.0        209.0          0.0  10039.0   \n",
       "2          64836.0       12482.0        197.0          0.0  21666.0   \n",
       "3          11596.0          36.0        662.0          0.0  43126.0   \n",
       "4          73050.0         135.0         54.0          0.0  15163.0   \n",
       "\n",
       "                                                text score_sign       u_days  \\\n",
       "0  He judged the hell out of you and decided you ...   positive  1276.865880   \n",
       "1               You may have saved his little life.    positive    48.880683   \n",
       "2                                    *Moisturize me*   positive  1764.699803   \n",
       "3  Frogs drink through their skin, so you cooled ...   positive  1871.736076   \n",
       "4             Its the happy wriggle that does me in.   positive   491.508611   \n",
       "\n",
       "   pca_score  label_neg-pos  label_neg-inv  label_bin  \n",
       "0   4.467621            2.0            0.0          0  \n",
       "1   5.000000            2.0            0.0          0  \n",
       "2   5.000000            2.0            0.0          0  \n",
       "3   5.000000            2.0            0.0          0  \n",
       "4   5.000000            2.0            0.0          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "# function to prepare text for NLP analysis\n",
    "def process_comment_text(comments, \n",
    "                         stemmer=None, \n",
    "                         regexstr=None, lowercase=True,\n",
    "                         removestop=False,\n",
    "                         verbose=True):\n",
    "    \"\"\"Helper function to pre-process text.\n",
    "        Combines several preprocessing steps: lowercase, \n",
    "            remove stop, regex text cleaning, stemming\"\"\"\n",
    "    \n",
    "    if type(stemmer) == str:\n",
    "        if stemmer.lower() == 'porter':\n",
    "            stemmer = PorterStemmer()\n",
    "        elif stemmer.lower() == 'snowball':\n",
    "            stemmer = SnowballStemmer(language='english')\n",
    "        else:\n",
    "            stemmer = None\n",
    "            \n",
    "    processed = comments\n",
    "    \n",
    "    # make text lowercase\n",
    "    if lowercase == True:\n",
    "        if verbose: print('make text lowercase')\n",
    "        processed = processed.str.lower()\n",
    "        \n",
    "    # remove stop words\n",
    "    # NOTE: stop words w/ capitals not removed!\n",
    "    if removestop == True:\n",
    "        if verbose: print('remove stop words')\n",
    "        stopwords = sw.words(\"english\")\n",
    "        processed = processed.map(lambda text: ' '.join([word for word in text.split() if word not in stopwords]))\n",
    "        \n",
    "    # apply regex expression\n",
    "    if regexstr is not None:\n",
    "        if verbose: print('apply regex expression')\n",
    "        regex = re.compile(regexstr) \n",
    "        processed = processed.str.replace(regex,' ')\n",
    "        \n",
    "    # stemming\n",
    "    # NOTE: stemming makes all lowercase\n",
    "    if stemmer is not None:\n",
    "        if verbose: print('stemming')\n",
    "        processed = processed.map(lambda x: ' '.join([stemmer.stem(y) for y in x.split(' ')]))\n",
    "        \n",
    "    if verbose: print('done')\n",
    "        \n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the X (pre-processed text) and y (label) variables for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make text lowercase\n",
      "apply regex expression\n",
      "stemming\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "processkwargs = {\n",
    "    'stemmer':'snowball', # snowball stemmer\n",
    "    'regexstr':'[^a-zA-Z0-9\\s]', # remove all but alphanumeric chars\n",
    "    'lowercase':True, # make lowercase\n",
    "    'removestop':False # don't remove stop words \n",
    "                }\n",
    "\n",
    "# the label used = 0-4 scale, w/ 4 = most toxic\n",
    "y = df['label_neg-inv']\n",
    "\n",
    "# process text, make that the text version of the training data\n",
    "verbose = True\n",
    "X_text = process_comment_text(df['text'], **processkwargs, verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the gridsearch \n",
    "\n",
    "The gridsearch using GridSearchCV is pretty straightforward: it takes a single estimator object, a set of parameters to test and some train/test data, and then exhaustively trains and tests the estimator with every parameter value combination to determine the one that gives the best score. [See here for more about hyperparameter tuning and gridsearch.](http://scikit-learn.org/stable/modules/grid_search.html#grid-search)\n",
    "\n",
    "The [Pipeline object](http://scikit-learn.org/stable/modules/pipeline.html#pipeline) is an estimator object that lets you chain multiple estimators so that you can transform data and train a classifier in one step. You can chain as many estimators as you want and even use your own custom estimator objects. In this case, I'll pipeline TfidfVectorizer and the classifier I'll use to classify NLP data. Once I create the pipeline object containing these two estimators, I can then pass that to GridSearchCV and tune both at the same time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Tfidf vectorizer:\n",
    "# define defaults: doing it this way allows us to define our own default params\n",
    "tfidfargs = {\n",
    "    \"analyzer\":'word', \n",
    "    \"max_features\" : None,\n",
    "    \"max_df\" : 0.25, # Filters out terms that occur in more than half of the docs (max_df=0.5)\n",
    "    \"min_df\" : 2, # Filters out terms that occur in only one document (min_df=2).\n",
    "    \"ngram_range\":(1, 3), # unigrams\n",
    "    \"stop_words\" : \"english\", # None, # \"english\", # Strips out “stop words”\n",
    "    \"use_idf\" : True\n",
    "    }\n",
    "\n",
    "# Logistic regression defaults:\n",
    "clfargs = {\n",
    "    \"penalty\":'l2', \n",
    "    \"class_weight\" : 'balanced',\n",
    "    \"solver\" : 'sag', # For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss\n",
    "    \"multi_class\" : 'multinomial', # alt: 'ovr'\n",
    "    \"n_jobs\": -1, # -1 = use all available CPU cores\n",
    "    }\n",
    "\n",
    "# Define a pipeline combining a text vectorizer with a Naive Bayes classifier\n",
    "pipeline = Pipeline([    \n",
    "    ('tfidf', TfidfVectorizer(**tfidfargs)),\n",
    "    ('clf', LogisticRegression(**clfargs)),\n",
    "])\n",
    "\n",
    "# Define the parameters and values we want to test.\n",
    "# Uncommenting more parameters will give better exploring power but will\n",
    "#   increase processing time in a combinatorial way. I suggest tuning <= 3\n",
    "#   parameters at a time.\n",
    "# Note the naming format: pipelineobjectname__paramname\n",
    "parameters = {\n",
    "    'tfidf__stop_words': ('english', None),\n",
    "    #'tfidf__analyzer': ('word', 'char', 'char_wb'),\n",
    "    #'tfidf__max_df': (0.1, 0.25, 0.5, 0.75),\n",
    "    #'tfidf__min_df': (1,2,5),\n",
    "    #'tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 3), (3, 3)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "}\n",
    "\n",
    "# create grid search object to find the best parameters for both the \n",
    "#   feature extraction and the classifier.\n",
    "# Note: n_jobs=-1 causes GridSearchCV to use multithreading to employ all processor cores.\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the grid search\n",
    "\n",
    "The gridsearch takes the pipeline object (containing the text vectorizer and the classifier) and the data and tries all combos of the parameters I have defined. The output parameter \"best\\_estimator\\_\" contains a pipeline object with the parameters that give best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'clf']\n",
      "parameters:\n",
      "{'tfidf__stop_words': ('english', None), 'tfidf__ngram_range': ((1, 1), (1, 3), (3, 3))}\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(X_text, y)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the best classifier using k-folds cross validation\n",
    "\n",
    "\n",
    "Cross_validate_classifier uses k-folds cross validation to partition the data into multiple non-overlapping train/test sets, and run the classifier on each. If the classifier is solid, the results should be the same for all sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# cross-validation of classifier model with text string data X_text, category labels in y\n",
    "def cross_validate_classifier(clf, X_text, y):\n",
    "\n",
    "    # set up kfold to generate several train-test sets, \n",
    "    #  with shuffled indices for selecting from data\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    i = 1\n",
    "    accuracy = []\n",
    "    for train_index, test_index in kf.split(X_text, y):\n",
    "        print(\"\\nk-fold train/test set #%d: \"%(i))\n",
    "\n",
    "        # fit the classifier with training data\n",
    "        clf.fit(X_text[train_index], y[train_index])\n",
    "\n",
    "        # generate predictions for test data\n",
    "        y_est = clf.predict(X_text[test_index])\n",
    "\n",
    "        # print results of the prediction test\n",
    "#         print_prediction_results(y_est, y[test_index])\n",
    "        accuracyscore = (y_est == y[test_index]).sum() / y_est.size\n",
    "        print(accuracyscore)\n",
    "\n",
    "        accuracy.append(accuracyscore)\n",
    "        i += 1\n",
    "\n",
    "    print(\"\\nOverall accuracy = %2.1f%%\"%(np.mean(accuracy)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate classifier using nonoverlapping subsets of data\n",
    "print(\"\\n***************************\")\n",
    "print(\"Cross-validate classifier:\")\n",
    "cross_validate_classifier(grid_search.best_estimator_, X_text, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
