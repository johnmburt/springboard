{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recommendation model 1: Item Search by Nearest Neighbors (ISNN)\n",
    "### Model hyperparameter tuning and evaluation\n",
    "\n",
    "### Springboard Capstone 2 project: building a recommendation engine\n",
    "### John Burt\n",
    "\n",
    "The goal of my project is to develop a working board game recommender using a dataset of explicit game ratings from the boardgamegeek.com forum site. My idea was to ask a user to name some games that they like, and my recommender algorithm will use those games to predict other games the user might like. The first problem I encountered was that any user asking for a recommendation will be unknown to the system - i.e., they are \"new users\" and that presents a classic cold start issue: how do you give a recommendation if you don't know much about the user? Well, we do have the user's \"liked games\" to work with, but any model will need to somehow match  that pattern of preferences to the preference patterns in the existing data.\n",
    "\n",
    "### Model description\n",
    "The first model I conceived estimates recommendations by creating a game (item) coordinate space. Each game is defined by a set of features/coordinates based on the dimensionally reduced ALS filled item x user ratings matrix (i.e, the utility matrix). Games rated similarly are closer in the game coordinate space, and so if a user likes one game, I can search for nearest neighbor games to use as recommendations.\n",
    "\n",
    "### Evalution function\n",
    "As with all models tested, I'm using a custom evaluation function to tune and assess the model:\n",
    "\n",
    "- Dataset is the unfilled user x game ratings matrix, train/test split by user.\n",
    "- For each test user:\n",
    "    - From N*2 top rated games, randomly select N as \"liked games\" to use as model input (X values).\n",
    "    - Remaining N top rated games assigned as holdouts to test for recommendations (y values).\n",
    "    - User score = # holdout games recommended / N\n",
    "    - The overall score is the mean user score for all test users.\n",
    "\n",
    "### Purpose of this notebook:\n",
    "\n",
    "Implement the the ISNN model, tune hyperparameters and evaluate model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# ---\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "srcdir = './data/'\n",
    "\n",
    "# load the unfilled item-user utility matrix\n",
    "df = pd.read_hdf(srcdir+'bgg_game_mx_unfilled.h5', 'mx')\n",
    "\n",
    "print('original: #games X #users:', df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the data for faster results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numusers = 30000\n",
    "numgames = 6000\n",
    "gameidx = np.array(range(numgames))\n",
    "np.random.shuffle(gameidx)\n",
    "useridx = np.array(range(numusers))\n",
    "np.random.shuffle(useridx)\n",
    "df = df.iloc[gameidx, useridx]\n",
    "print('reduced: #games X #users:', df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train/test sets\n",
    "\n",
    "Split by user, so that all ratings by a given user are either in the train or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# transpose so users = rows\n",
    "df_u = df.T\n",
    "\n",
    "# split into train and test sets\n",
    "train, test = train_test_split(df_u, test_size=.1 )\n",
    "\n",
    "# for training set, put items back on rows\n",
    "train = train.T\n",
    "\n",
    "print('train set:',train.shape, '  test set:',test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split test into X ('liked' games) and y (target)\n",
    "\n",
    "- From N*2 top rated games, select N as \"liked games\" to use as model input (X values).\n",
    "- Remaining N top rated games assigned as holdouts to test for recommendations (y values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_top = 20\n",
    "n_liked = int(n_top/2)\n",
    "n_recs = n_liked\n",
    "\n",
    "test_X = np.zeros([test.shape[0],n_liked])\n",
    "test_y = np.zeros([test.shape[0],n_top])\n",
    "game_ids = test.columns.values\n",
    "\n",
    "for i in range(test.shape[0]):\n",
    "    row = test.iloc[i,:].values # row=1 user's ratings (including nans)\n",
    "    # get indices to descending sort of ratings (nans sorted to bottom)\n",
    "    idx = np.argsort(-row)\n",
    "    # top n_top+n_liked highest rated game IDs\n",
    "    top = game_ids[idx[:(n_top+n_liked)]]\n",
    "    # randomize order\n",
    "    np.random.shuffle(top)\n",
    "    # assign to X (liked games) and y(games to test for recced)\n",
    "    test_X[i,:] = top[:n_liked]\n",
    "    test_y[i,:] = top[n_liked:]\n",
    "\n",
    "print(test_X.shape, test_y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement recommender as sklearn estimator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# import utility functions\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "import recsys_utilities\n",
    "import importlib\n",
    "importlib.reload(recsys_utilities)\n",
    "from recsys_utilities import do_ALS_df\n",
    "\n",
    "# Item Space Nearest Neighbor Search\n",
    "class Recommender_ISNNS(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"recommender engine as an estimator\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                regularization=0.1, \n",
    "                n_factors=100,\n",
    "                n_iterations=8,\n",
    "                scale=True,\n",
    "                weighted=True,\n",
    "                bm25_K1=10,\n",
    "                bm25_B=0.8,\n",
    "                n_neighbors=10, \n",
    "                n_SVD_dims=100,\n",
    "                verbose=False):\n",
    "        \"\"\"\n",
    "        Called when initializing the model\n",
    "        \"\"\"\n",
    "        # model parameters\n",
    "        self.regularization = regularization\n",
    "        self.n_factors = n_factors\n",
    "        self.n_iterations = n_iterations\n",
    "        self.scale = scale\n",
    "        self.weighted = weighted\n",
    "        self.bm25_K1 = bm25_K1\n",
    "        self.bm25_B = bm25_B\n",
    "        \n",
    "        self.n_neighbors = n_neighbors  # number of neighbor titles to search for\n",
    "        self.n_SVD_dims = n_SVD_dims\n",
    "        \n",
    "        self.verbose = verbose\n",
    "\n",
    "        # internal data used for making recommendations\n",
    "        self.util_mx = None\n",
    "\n",
    "    # ******************************************************************\n",
    "    def set_params(self, **params):\n",
    "        self.__dict__.update(params)\n",
    "\n",
    "    # ******************************************************************\n",
    "    def do_ALS(self, X):\n",
    "        \"\"\"Fill utility matrix with ALS inferred ratings\"\"\"\n",
    "        self.util_mx, item_factors, user_factors = do_ALS_df(\n",
    "            X, ALS_method='implicit', \n",
    "            n_iterations=self.n_iterations, \n",
    "            regularization=self.regularization, \n",
    "            n_factors=self.n_factors, \n",
    "            verbose=self.verbose,\n",
    "            scale=self.scale,\n",
    "            weighted=self.weighted,\n",
    "            bm25_K1=self.bm25_K1,\n",
    "            bm25_B=self.bm25_B,\n",
    "            use_native=True,\n",
    "            use_cg=True,\n",
    "            use_gpu=False,\n",
    "            )   \n",
    "    \n",
    "    # ******************************************************************\n",
    "    def make_SVD_features(self):\n",
    "        \"\"\"Generate feature coordinates for items (rows)\"\"\"\n",
    "        # create item coordinate space with Truncated SVD\n",
    "        # select all columns after 'gameID'\n",
    "#         self.item_coords = TruncatedSVD(n_components=self.n_SVD_dims).fit_transform(\n",
    "#             self.util_mx.values)\n",
    "        self.item_coords = PCA(n_components=self.n_SVD_dims, whiten=True).fit_transform(\n",
    "            self.util_mx.values)\n",
    "\n",
    "    # ******************************************************************\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\" Train the recommender.\n",
    "            This means, fill the item matrix with estimated ratings using ALS.\n",
    "            You can also pass a filled matrix (has no empty cells / NaN values),\n",
    "              and the fit function will skip the ALS step.\n",
    "            X = pd DataFrame filled/unfilled utility matrix\n",
    "        \"\"\"\n",
    "        # do ALS to fill in the empty rating cells\n",
    "        self.do_ALS(X)\n",
    "\n",
    "        # create item coordinate space with PCA (Truncated SVD)\n",
    "        self.make_SVD_features()\n",
    "        \n",
    "        # create array of game IDs\n",
    "        self.item_IDs = np.array(X.index)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # ******************************************************************\n",
    "    def find_nearest_neighbors(self, coords, x, numnearest):\n",
    "        \"\"\"Brute force nearest neighbor search\"\"\"\n",
    "        \n",
    "        # get euclidean distances of all points to x\n",
    "        dists = cdist(x, coords)\n",
    "        \n",
    "        # sort the distances\n",
    "        ind, = np.argsort(dists)\n",
    "\n",
    "        # return the numnearest nearest neighbors\n",
    "        return ind[:numnearest]\n",
    "\n",
    "    # ******************************************************************\n",
    "    def recommend_games_one(self, target_ID, num2rec=1):\n",
    "        \"\"\"Recommend games based on nearest neighbor to one game title\"\"\"\n",
    "\n",
    "        # get coords of target title,\n",
    "        targetcoord = self.item_coords[self.item_IDs == target_ID, :]\n",
    "\n",
    "        # didn't find the target ID in the ID list\n",
    "        if targetcoord.shape[0] == 0:\n",
    "            return []\n",
    "        \n",
    "        # find nearest neighbors\n",
    "        ind = self.find_nearest_neighbors(self.item_coords, targetcoord, \n",
    "                                          max(self.n_neighbors, num2rec+1))\n",
    "\n",
    "        # Note: first entry will be the target title (distance 0)\n",
    "        return ind[1:num2rec+1]\n",
    "\n",
    "    # ******************************************************************\n",
    "    def recommend_games_by_pref_list(self, liked_IDs, num2rec=10): \n",
    "        \n",
    "        \"\"\"Recommend games using multiple liked games in a list of titles.\n",
    "           This method creates a set of recommended games for each title in prefs and\n",
    "             then selects the most commonly recommended ones for the final rec list\"\"\"\n",
    "\n",
    "        # collect recommended games for each liked game ID\n",
    "        recs = []\n",
    "        for id in liked_IDs:\n",
    "            recs.extend(self.recommend_games_one(id, num2rec))\n",
    "            \n",
    "        # sort recommended game IDs by counts, select IDs with highest counts.\n",
    "        # NOTE: np.unique sorts the results, which could create a bias for older games \n",
    "        unique, counts = np.unique(recs, return_counts=True)\n",
    "        recs = (np.array([unique, counts])[0, np.argsort(-counts)].T)\n",
    "\n",
    "        return recs[:num2rec]\n",
    "\n",
    "    # ******************************************************************\n",
    "    def predict(self, X, y=None, num2rec=10):\n",
    "        \"\"\"predict == recommend board IDs = y\n",
    "        X = array of liked game IDs\n",
    "        \"\"\"\n",
    "\n",
    "        # recommend game IDs for each row of liked games in X\n",
    "        y = np.zeros([X.shape[0], num2rec])\n",
    "        for i in range(X.shape[0]):\n",
    "            y[i,:] = self.recommend_games_by_pref_list(X[i,:], num2rec=num2rec)            \n",
    "\n",
    "        return y\n",
    "\n",
    "    # ******************************************************************\n",
    "    def score(self, y_true, y_pred):\n",
    "        \"\"\"mean percent of y_true game IDs in y_pred\"\"\"\n",
    "        \n",
    "        n_recced = y_pred.shape[1]\n",
    "        \n",
    "        result = np.zeros([y_true.shape[0]])\n",
    "        for i,(x,y) in enumerate(zip(y_true, y_pred)):\n",
    "            result[i] = len(set(x).intersection(y))/n_recced\n",
    "        \n",
    "        # counts number of values bigger than mean\n",
    "        return np.mean(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the recommender model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = Recommender_ISNNS(**defaults)\n",
    "rec.fit(train)\n",
    "y_pred = rec.predict(test_X, num2rec=n_recs)\n",
    "score = rec.score(test_y, y_pred)\n",
    "print('score =', score) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization using Baysian methods\n",
    "\n",
    "I'm using the hyperparameter tuning package [hyperopt](https://github.com/hyperopt/hyperopt).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from time import time\n",
    "from hyperopt import tpe, hp, fmin, Trials\n",
    "from time import time\n",
    "from hyperopt import space_eval\n",
    "\n",
    "# model defaults\n",
    "defaults = {\n",
    "    'n_factors': 75,\n",
    "    'weighted': False,\n",
    "    'bm25_K1': 4,\n",
    "    'bm25_B': 0.5,\n",
    "    'n_SVD_dims': 20,\n",
    "    'n_neighbors': 10,\n",
    "    'n_iterations': 10,\n",
    "    'verbose': False,\n",
    "    'scale': True,\n",
    "    'regularization': 0.3,\n",
    "    }\n",
    " \n",
    "# hyperopt parameter space\n",
    "# param space if B25 weighting isn't used\n",
    "paramspace = {\n",
    "    'n_factors': 2+hp.randint('n_factors', 100),\n",
    "    'n_SVD_dims': 2+hp.randint('n_SVD_dims', 100),\n",
    "    'n_neighbors': 5+hp.randint('n_neighbors', 200),\n",
    "#     'regularization': hp.uniform('regularization', 0.01, 1.0),\n",
    "    }\n",
    "\n",
    "# # param space if I use BM25 weighting\n",
    "# paramspace = {\n",
    "#     'n_factors': 10+hp.randint('n_factors', 200),\n",
    "#     'n_SVD_dims': 5+hp.randint('n_SVD_dims', 200),\n",
    "#     'n_neighbors': 5+hp.randint('n_neighbors', 200),\n",
    "#     'BM25 weighted': hp.choice('BM25 weighted', [\n",
    "#         {\n",
    "#             'weighted': True,\n",
    "#             'bm25_K1': 1+hp.randint('bm25_K1', 10),\n",
    "#             'bm25_B': hp.uniform('bm25_B', 0.1, 1.0),\n",
    "#         },\n",
    "#         {\n",
    "#             'weighted': False,\n",
    "#         }\n",
    "#     ]) }\n",
    "                       \n",
    "# hyperopt objective function\n",
    "def objective(params):\n",
    "#     global train, test_X, test_y\n",
    "    rec = Recommender_ISNNS(**defaults)\n",
    "    rec.set_params(**params)\n",
    "    rec.fit(train)\n",
    "    y_pred = rec.predict(test_X, num2rec=n_recs)\n",
    "    score = rec.score(test_y, y_pred)\n",
    "    return 1-score\n",
    "\n",
    "# hyperparameter tuning:\n",
    "# The Trials object will store details of each iteration\n",
    "trials = Trials()\n",
    "    \n",
    "# Run the hyperparameter search using the tpe algorithm\n",
    "t0 = time()\n",
    "print('  tune model')\n",
    "best = fmin(fn=objective, space=paramspace, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "print('    done in %0.3fs,'%(time() - t0))\n",
    "\n",
    "# Get the values of the optimal parameters\n",
    "best_params = space_eval(paramspace, best)\n",
    "print('\\n  Best parameters:',best_params)   \n",
    "\n",
    "rec = Recommender_ISNNS(**defaults)\n",
    "rec.set_params(**best_params)\n",
    "rec.fit(train)\n",
    "y_pred = rec.predict(test_X, num2rec=n_recs)\n",
    "score = rec.score(test_y, y_pred)\n",
    "print('score =', score) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt.plotting\n",
    "\n",
    "plt.figure( figsize=(10,5))\n",
    "hyperopt.plotting.main_plot_history(trials)\n",
    "plt.figure( figsize=(10,5))\n",
    "hyperopt.plotting.main_plot_histogram(trials)\n",
    "plt.figure( figsize=(15,5))\n",
    "hyperopt.plotting.main_plot_vars(trials)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot performance of hyperparams across the range tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# only get x and y vals for plotting if there is an associated \n",
    "#  loss value. \n",
    "#  Additionally, need to deal with nested results \n",
    "def get_plot_vals(results, paramname):\n",
    "    x_val = [t['tid'] for t in results]\n",
    "    y_val = [t['misc']['vals'][paramname] for t in results]\n",
    "    # list of lists means some entries are empty\n",
    "    if type(y_val[0]) is list: \n",
    "        ytemp = np.array([y[0] if y else np.nan for y in y_val])\n",
    "        y_val = np.array(ytemp)[~np.isnan(ytemp)]\n",
    "        x_val = np.array(x_val)[~np.isnan(ytemp)]\n",
    "    x_loss = [t['misc']['vals'][paramname] for t in results]\n",
    "    y_loss = [t['result']['loss'] for t in results]\n",
    "    # list of lists means some entries are empty\n",
    "    if type(x_loss[0]) is list:\n",
    "        xtemp = np.array([x[0] if x else np.nan for x in x_loss])\n",
    "        y_loss = np.array(y_loss)[~np.isnan(xtemp)]\n",
    "        x_loss = np.array(xtemp)[~np.isnan(xtemp)]\n",
    "    return x_val, y_val, x_loss, y_loss\n",
    "\n",
    "# if 'loss' not in results[-1]['result']:\n",
    "#     results = results[:-1]\n",
    "\n",
    "results = trials.trials[:-1]\n",
    "# results = trials.trials\n",
    "times = [t['tid'] for t in results]\n",
    "\n",
    "for paramname in results[0]['misc']['vals'].keys():\n",
    "    x_val, y_val, x_loss, y_loss = get_plot_vals(results, paramname)\n",
    "    \n",
    "    f, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "    xs = times\n",
    "    # param value over time\n",
    "    ax[0].set_xlim(xs[0]-10, xs[-1]+10)\n",
    "    ax[0].scatter(x_val, y_val, s=20, linewidth=0.01, alpha=0.75)\n",
    "    ax[0].set_title('%s vs t '%(paramname), fontsize=18)\n",
    "    ax[0].set_xlabel('t', fontsize=16)\n",
    "    ax[0].set_ylabel(paramname, fontsize=16)\n",
    "    \n",
    "    # loss vs param val\n",
    "    ax[1].set_ylim(min(y_loss)-.001, max(y_loss)+.001)\n",
    "    ax[1].scatter(x_loss, y_loss, s=20, linewidth=0.01, alpha=0.75)\n",
    "    ax[1].set_title('loss vs %s '%(paramname), fontsize=18)\n",
    "    ax[1].set_xlabel(paramname, fontsize=16)\n",
    "    ax[1].set_ylabel('loss', fontsize=16)\n",
    "    plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
