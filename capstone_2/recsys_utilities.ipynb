{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project utility functions\n",
    "\n",
    "### Springboard Capstone 2 project: building a recommendation engine\n",
    "### John Burt\n",
    "\n",
    "\n",
    "### Purpose of this notebook:\n",
    "\n",
    "Collect several functions that get used by more than one notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares method\n",
    "\n",
    "About this code: I originally used an ALS algorithm written by my friend and colleague Matt Borthwick, but then I discovered the [OSS project implicit](https://implicit.readthedocs.io/en/latest/als.html), which implemented a much faster ALS algorithm. I've kept Matt's code here because it might come in handy some time, but for the project I always use implicit now. The entry function, do_ALS_df allows you to specify which version of the algorithm to use.\n",
    "\n",
    "For more about the optimizations used by implicit's ALS method, see [Applications of the Conjugate Gradient Method for ImplicitFeedback Collaborative Filtering](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.379.6473&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matt Borthwick's implementation of Alternating Least Squares\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from numpy import eye\n",
    "from numpy.linalg import solve\n",
    "from numpy.random import rand\n",
    "from scipy import sparse\n",
    "\n",
    "#*****************************************************\n",
    "# ALS calculation using Matt Borthwick's implementation\n",
    "def do_ALS_MB(iu_mx_sparse, \n",
    "              n_factors=4, \n",
    "              n_iterations=10, \n",
    "              regularization=0.01, \n",
    "              weighted=True, \n",
    "              verbose=True ):\n",
    "    #    unrated items should be recorded as zero\n",
    "    #\n",
    "    #    iu_mx_sparse should have an element-wise multiply method, an element-wise minimum method, \n",
    "    #    and a shape attribute, like scipy.sparse matrices do\n",
    "    if verbose: \n",
    "        print(\"setting up matrices, %d iterations: \"%(n_iterations),end='')\n",
    "    n_users, n_items = iu_mx_sparse.shape\n",
    "    X = rand(n_users, n_factors)\n",
    "    Y_T = rand(n_items, n_factors)\n",
    "    r = iu_mx_sparse.minimum(1)\n",
    "    if regularization:\n",
    "        regularization *= eye(n_factors)\n",
    "    for iteration in range(1, n_iterations+1):\n",
    "        if verbose: print(\"%d,\"%(iteration),end='')\n",
    "        for u in range(n_users):\n",
    "            A = r[u].multiply(Y_T.T) @ Y_T\n",
    "            b = iu_mx_sparse[u] @ Y_T\n",
    "            X[u] = solve(A + regularization, b[0])\n",
    "        for i in range(n_items):\n",
    "            A = (r[:, i].multiply(X)).T @ X\n",
    "            b = iu_mx_sparse[:, i].T @ X\n",
    "            Y_T[i] = solve(A + regularization, b[0])\n",
    "#     Q_hat = np.dot(X,Y_T.T)\n",
    "    \n",
    "    return X, Y_T\n",
    "\n",
    "\n",
    "#*****************************************************\n",
    "# ALS calculation using the implicit ALS package\n",
    "import os  \n",
    "import implicit\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "\n",
    "def do_ALS_implicit(iu_mx_sparse, \n",
    "                        n_factors=4, \n",
    "                        n_iterations=10, \n",
    "                        regularization=0.01, \n",
    "                        weighted=True, \n",
    "                        verbose=True ,\n",
    "                        bm25_K1=10,\n",
    "                        bm25_B=0.8,\n",
    "                        use_native=True, \n",
    "                        use_cg=True, \n",
    "                        use_gpu=False, \n",
    "                   ):\n",
    "    \n",
    "    os.environ['MKL_NUM_THREADS'] = '1'\n",
    "    \n",
    "    als = AlternatingLeastSquares(factors=n_factors, \n",
    "            regularization=regularization, use_native=use_native, \n",
    "            use_cg=use_cg, use_gpu=use_gpu, iterations=n_iterations,\n",
    "            calculate_training_loss=True, num_threads=0)\n",
    "    \n",
    "    if verbose: \n",
    "        print('fitting ALS model')\n",
    "        \n",
    "    if weighted:\n",
    "        als.fit(bm25_weight(sparse.csr_matrix(np.nan_to_num(iu_mx_sparse)), \n",
    "                            K1=bm25_K1, B=bm25_B), show_progress=verbose)\n",
    "    else:\n",
    "        als.fit(sparse.csr_matrix(np.nan_to_num(iu_mx_sparse)), show_progress=verbose)\n",
    "    \n",
    "    return als.item_factors, als.user_factors\n",
    "\n",
    "#*****************************************************\n",
    "def scale_to_mx(mx1, mx2):\n",
    "    \"\"\"Scale mx2 to mx1's mean and std\"\"\"\n",
    "    return ((mx2-np.nanmean(mx2)) * (np.nanstd(mx1)/np.nanstd(mx2)) + \n",
    "            np.nanmean(mx1))\n",
    "    \n",
    "#*****************************************************\n",
    "def do_ALS_df(df, ALS_method='implicit', scale=True, return_utilmx=True, **args):\n",
    "    \"\"\"run ALS on array to fill in NaN cells \n",
    "    with a useful iu_mx_sparse estimate:\n",
    "    user_factors = user hidden factors\n",
    "    item_factors = item hidden factors\n",
    "    \"\"\"\n",
    "\n",
    "    if ALS_method != 'implicit':\n",
    "        item_factors, user_factors = do_ALS_MB(\n",
    "            sparse.csr_matrix(np.nan_to_num(df.values)), **args)\n",
    "    else:\n",
    "        item_factors, user_factors = do_ALS_implicit(\n",
    "            sparse.csr_matrix(np.nan_to_num(df.values)), **args)\n",
    "        \n",
    "    if not return_utilmx:\n",
    "        return item_factors, user_factors\n",
    "\n",
    "    else:\n",
    "        if scale:\n",
    "            return (pd.DataFrame( scale_to_mx(df.values, np.dot(item_factors, user_factors.T)),\n",
    "                index=df.index, columns=df.columns), item_factors, user_factors)\n",
    "        else:\n",
    "            return (pd.DataFrame( np.dot(item_factors, user_factors.T),\n",
    "                index=df.index, columns=df.columns), item_factors, user_factors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcs to generate train / test datasets for models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split test into X ('liked' games) and y (target)\n",
    "\n",
    "- From n_liked+n_recs top rated games, randomly select n_liked as \"liked games\" to use as model input (X values).\n",
    "- Remaining n_recs top rated games assigned as holdouts to test for recommendations (y values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_Xy(test, n_liked = 10, n_recs = 10 ):\n",
    "    \"\"\"Split top rated item for users into X and y item ID arrays\"\"\"\n",
    "\n",
    "    # total number of top ratings to split between n_liked & n_recs\n",
    "    n_top = n_liked + n_recs\n",
    "    \n",
    "    # arrays to hold X and y item IDs\n",
    "    test_X = np.zeros([test.shape[0],n_liked])\n",
    "    test_y = np.zeros([test.shape[0],n_recs])\n",
    "    item_ids = test.columns.values\n",
    "\n",
    "    # for each user (row), select top n_top items and \n",
    "    #  split them into X and y categories\n",
    "    for i in range(test.shape[0]):\n",
    "        # row = a user's ratings (including nans)\n",
    "        row = test.iloc[i,:].values \n",
    "        # get indices to descending sort of ratings (nans sorted to bottom)\n",
    "        idx = np.argsort(-row)\n",
    "        # top n_top highest rated item IDs\n",
    "        top = item_ids[idx[:(n_top)]]\n",
    "        # randomize order\n",
    "        np.random.shuffle(top)\n",
    "        # assign to X (liked items) and y(items to test for recced)\n",
    "        test_X[i,:] = top[:n_liked]\n",
    "        test_y[i,:] = top[n_liked:]\n",
    "    \n",
    "    return (pd.DataFrame(test_X,index=test.index), \n",
    "            pd.DataFrame(test_y,index=test.index) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train/test sets\n",
    "\n",
    "Split by user, so that all ratings by a given user are either in the train or test set. \n",
    "\n",
    "Test sets are further split into X and y sets of top rated game IDs\n",
    "\n",
    "NOTE: the df passed must be in format users (rows) x items (cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_test_split_utilitymx(df, \n",
    "                                test_size = 0.1,\n",
    "                                n_liked = 10,\n",
    "                                n_recs = 10 ):\n",
    "    \"\"\"Split utility matrix df into train and test sets.\n",
    "        df is format: items (rows) x user (cols) x ratings.\n",
    "        \n",
    "        Output is:\n",
    "            train = user x item rating matrix\n",
    "            test_X = n_liked top rated item IDs\n",
    "            test_y = n_recs top rated item IDs\"\"\"\n",
    "\n",
    "    # split into train and test sets\n",
    "    train, test = train_test_split(df, test_size=test_size )\n",
    "\n",
    "    # set train and test index to userID\n",
    "    train = train.set_index('userID')\n",
    "    test = test.set_index('userID')\n",
    "    \n",
    "    # convert the test ratings into X and y matrices\n",
    "    test_X, test_y = get_test_Xy(test, \n",
    "                    n_liked = n_liked,\n",
    "                    n_recs = n_recs )\n",
    "    \n",
    "    return train, test, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
